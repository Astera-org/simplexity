<context>
# Overview  
This library is used to training models (typically transformers or RNNs) on sequences of data generated by HMMs.

# Core Features  
## 1. Generative Process Simulation
- **Hidden Markov Models (HMMs)**: Full implementation of discrete HMMs with configurable transition matrices and observation spaces
- **Generalized HMMs**: Extended framework supporting more complex state representations and transitions
- **Mixed State Presentations**: Advanced state representation capabilities for complex probabilistic models
- **Configurable Data Generation**: Parameterizable sequence generation with controllable complexity and structure

**Why it's important**: Provides ground truth data with known statistical properties for evaluating how well neural models learn underlying structures.

**How it works**: Uses JAX-based implementations of stochastic processes to generate sequences with known computational mechanics properties.

## 2. Neural Predictive Models  
- **GRU-based RNNs**: Modular GRU implementations with configurable architecture (embedding size, hidden layers, etc.)
- **Transformer Support**: Framework ready for transformer architectures (currently RNN-focused)
- **Extensible Model Interface**: Clean abstraction allowing easy addition of new model types

**Why it's important**: Enables systematic comparison of different neural architectures on controlled sequence prediction tasks.

**How it works**: Equinox-based neural network implementations with JAX for efficient training and evaluation.

## 3. Training and Evaluation Framework
- **Hydra Configuration**: Systematic experiment configuration and management
- **Optuna Integration**: Automated hyperparameter optimization and multi-run experiments  
- **MLFlow Logging**: Comprehensive experiment tracking and model versioning
- **Checkpointing**: Robust model persistence with local and S3 storage options

**Why it's important**: Enables reproducible research and systematic exploration of model performance across different conditions.

**How it works**: Configuration-driven training pipeline with automatic logging and checkpointing.

## 4. Computational Mechanics Analysis
- **Sequence Probability Calculation**: Exact computation of sequence probabilities under generative models
- **Belief State Tracking**: Real-time tracking of posterior distributions over hidden states
- **Model Comparison**: Tools for comparing neural model predictions against ground truth generative processes

**Why it's important**: Bridges the gap between traditional computational mechanics theory and modern neural sequence models.

**How it works**: Mathematical analysis tools that compute theoretical properties and compare them with learned model behavior.
</context>
<PRD>
# User Experience  
## User Personas
### 1. Computational Mechanics Researchers
- **Goals**: Study how neural networks learn to represent hidden structure in sequences
- **Needs**: Controlled experimental environments, theoretical analysis tools, reproducible results
- **Experience Level**: Advanced knowledge of stochastic processes and machine learning

### 2. Machine Learning Researchers  
- **Goals**: Benchmark sequence models on well-characterized tasks
- **Needs**: Easy model training, hyperparameter optimization, clear evaluation metrics
- **Experience Level**: Strong ML background, may be new to computational mechanics

### 3. Graduate Students
- **Goals**: Learn about sequence modeling and computational mechanics through hands-on experimentation
- **Needs**: Clear examples, educational notebooks, well-documented APIs
- **Experience Level**: Intermediate, learning advanced concepts

## Key User Flows
### 1. Basic Research Workflow
1. Configure generative process (HMM parameters)
2. Generate training/test sequences  
3. Configure neural model architecture
4. Train model with automatic logging
5. Analyze learned representations vs. ground truth

### 2. Comparative Study Workflow
1. Define multiple experimental conditions
2. Run hyperparameter sweeps with Optuna
3. Compare results across models/conditions
4. Generate analysis notebooks and visualizations

### 3. Educational Workflow
1. Explore provided example notebooks
2. Modify parameters to see effects
3. Visualize learning dynamics and model behavior
4. Understand connections between theory and practice

## UI/UX Considerations
- **Configuration-Driven**: Minimal code changes needed for new experiments
- **Notebook-Friendly**: Designed for interactive exploration and visualization
- **Reproducible**: All experiments fully specified by configuration files
- **Extensible**: Clear interfaces for adding new models and generative processes

# Technical Architecture  
## System Components
### 1. Core Library (`simplexity/`)
- **Generative Processes**: HMM implementations and data generation
- **Predictive Models**: Neural network architectures (currently GRU-based RNNs)
- **Training**: Model training loops and optimization
- **Evaluation**: Model assessment and comparison tools
- **Persistence**: Model checkpointing and loading
- **Logging**: Experiment tracking and metrics collection

### 2. Configuration System (`simplexity/configs/`)
- **Hydra-based**: Hierarchical configuration management
- **Experiment Configs**: Multi-run experiment specifications
- **Model Configs**: Architecture and hyperparameter definitions
- **Data Configs**: Generative process parameterization

### 3. Analysis Tools (`notebooks/`)
- **Training Notebooks**: Interactive model training and monitoring
- **Visualization**: Model inspection and analysis tools
- **Benchmarking**: Performance comparison utilities

## Data Models
### 1. Generative Process State
```python
State = jax.Array  # Probability distribution over hidden states
```

### 2. Sequence Data
```python
Sequence = jax.Array  # Integer sequences representing observations
```

### 3. Model Parameters
- **Transition Matrices**: `(vocab_size, num_states, num_states)` arrays
- **Neural Weights**: Equinox module parameters with JAX arrays

## APIs and Integrations
### 1. Internal APIs
- **GenerativeProcess**: Abstract interface for sequence generation
- **PredictiveModel**: Abstract interface for neural models
- **ModelPersister**: Abstract interface for model storage

### 2. External Integrations
- **JAX**: Core computation and automatic differentiation
- **Equinox**: Neural network framework
- **Hydra**: Configuration management
- **MLFlow**: Experiment tracking
- **Optuna**: Hyperparameter optimization
- **S3**: Cloud storage for model checkpoints

## Infrastructure Requirements
### 1. Compute Requirements
- **GPU Support**: CUDA-enabled JAX for neural model training
- **Memory**: Depends on model size and sequence length
- **Python 3.12+**: Modern Python with type hints

### 2. Storage Requirements
- **Local**: Experiment logs, model checkpoints, generated data
- **Cloud (Optional)**: S3 for persistent model storage and sharing

### 3. Dependencies
- **Core**: JAX, Equinox, Hydra, MLFlow, Optuna
- **Data**: NumPy, Pandas for data manipulation  
- **Visualization**: Matplotlib, Jupyter for analysis
- **Development**: Pytest, Ruff, PyRight for code quality

# Development Roadmap  
## Phase 1: Core Foundation (MVP)
### 1.1 Basic HMM Implementation ✅
- Discrete Hidden Markov Models with configurable parameters
- Sequence generation from HMMs
- Probability calculations and belief state tracking

### 1.2 GRU-based RNN Models ✅  
- Modular GRU architecture with embedding layers
- Configurable hidden dimensions and layer counts
- JAX/Equinox implementation for efficient training

### 1.3 Training Pipeline ✅
- Hydra configuration system
- Basic model training with checkpointing
- MLFlow integration for experiment logging

### 1.4 Local Development Environment ✅
- UV-based dependency management
- Testing framework with pytest
- Code quality tools (ruff, pyright)

## Phase 2: Enhanced Capabilities
### 2.1 Advanced Generative Models
- **Generalized HMMs**: More flexible state representations
- **Mixed State Presentations**: Complex probabilistic state models
- **Configurable Complexity**: Parameterizable model complexity

### 2.2 Transformer Architecture Support
- **Attention Mechanisms**: Transformer encoder implementations
- **Positional Encoding**: Support for sequence position information
- **Architecture Comparison**: Direct comparison with RNN models

### 2.3 Advanced Training Features
- **Optuna Integration**: Automated hyperparameter optimization
- **Multi-run Experiments**: Systematic hyperparameter sweeps
- **Distributed Training**: Multi-GPU and multi-node support

### 2.4 Cloud Infrastructure
- **S3 Persistence**: Cloud-based model storage and sharing
- **Remote Training**: Cloud-based training infrastructure
- **Collaborative Features**: Shared experiment tracking

## Phase 3: Analysis and Visualization
### 3.1 Computational Mechanics Analysis
- **Causal State Reconstruction**: Infer computational structure from neural models
- **Information Theoretic Measures**: Entropy, mutual information, excess entropy
- **Model Comparison Metrics**: Quantitative comparison of learned vs. true structure

### 3.2 Interactive Visualization
- **Real-time Training Monitoring**: Live training progress visualization  
- **Model Inspection Tools**: Interactive exploration of learned representations
- **Comparative Analysis**: Side-by-side model comparison interfaces

### 3.3 Educational Resources
- **Tutorial Notebooks**: Step-by-step learning materials
- **Example Gallery**: Curated examples for different use cases
- **Documentation**: Comprehensive API and conceptual documentation

## Phase 4: Advanced Research Features  
### 4.1 Extended Model Support
- **VAE Integration**: Variational autoencoders for sequence modeling
- **Normalizing Flows**: Continuous latent variable models
- **Hybrid Architectures**: Combined discrete/continuous models

### 4.2 Large-Scale Experimentation
- **Automated Experiment Design**: AI-assisted experiment planning
- **Meta-Learning**: Learning across multiple generative processes
- **Continual Learning**: Sequential learning from multiple tasks

### 4.3 Production Integration
- **Model Serving**: Deploy trained models as web services
- **Real-time Inference**: Low-latency sequence prediction
- **Model Monitoring**: Production model performance tracking

# Logical Dependency Chain
## Foundation Layer (Required First)
1. **Core HMM Implementation** → Provides ground truth data generation
2. **Basic Neural Models** → Enables model training and evaluation  
3. **Training Infrastructure** → Supports systematic experimentation
4. **Configuration System** → Enables reproducible research

## Enhancement Layer (Build Upon Foundation)
5. **Advanced Generative Models** → Extends experimental capabilities
6. **Hyperparameter Optimization** → Improves model performance systematically
7. **Cloud Infrastructure** → Enables large-scale experimentation
8. **Transformer Support** → Provides modern architecture comparison

## Analysis Layer (Requires Enhanced Capabilities)  
9. **Computational Mechanics Analysis** → Core research contribution
10. **Visualization Tools** → Makes results interpretable and accessible
11. **Educational Resources** → Enables knowledge transfer and adoption

## Advanced Layer (Research Extensions)
12. **Extended Model Support** → Pushes research boundaries
13. **Large-Scale Experimentation** → Enables comprehensive studies
14. **Production Integration** → Translates research to applications

## MVP Success Criteria
- ✅ Generate sequences from configurable HMMs
- ✅ Train GRU models on generated sequences  
- ✅ Compare model predictions to ground truth probabilities
- ✅ Track experiments with MLFlow
- ✅ Reproduce results from configuration files

## Immediate Next Steps (Ready for Development)
1. **Transformer Architecture Implementation**
2. **Enhanced Visualization Tools**  
3. **Computational Mechanics Analysis Functions**
4. **Educational Notebook Creation**

# Risks and Mitigations  
## Technical Challenges
### 1. Computational Complexity
- **Risk**: Large models and long sequences may be computationally expensive
- **Mitigation**: JAX JIT compilation, efficient memory management, cloud scaling options

### 2. Numerical Stability  
- **Risk**: Log probability calculations may suffer from numerical issues
- **Mitigation**: LogSumExp tricks, careful numerical implementation, extensive testing

### 3. Model Convergence
- **Risk**: Neural models may not learn the underlying HMM structure
- **Mitigation**: Careful architecture design, hyperparameter tuning, regularization techniques

## Research and Development Risks
### 1. Limited Model Diversity
- **Risk**: Currently only supports GRU-based RNNs, limiting comparative analysis
- **Mitigation**: Prioritize transformer implementation, modular architecture for easy extension

### 2. Scalability Limitations
- **Risk**: Framework may not scale to very large models or datasets
- **Mitigation**: JAX's scalability features, distributed training support, cloud integration

### 3. Reproducibility Challenges
- **Risk**: Complex stochastic processes may be difficult to reproduce exactly
- **Mitigation**: Careful random seed management, deterministic algorithms where possible, comprehensive logging

## Adoption and Usability Risks
### 1. Steep Learning Curve
- **Risk**: Requires knowledge of both computational mechanics and modern ML
- **Mitigation**: Comprehensive documentation, tutorial notebooks, example gallery

### 2. Limited Community
- **Risk**: Niche research area may have limited user base
- **Mitigation**: Focus on educational value, clear examples, broader ML community outreach

### 3. Configuration Complexity  
- **Risk**: Hydra configuration system may be overwhelming for new users
- **Mitigation**: Sensible defaults, example configurations, progressive complexity in documentation

# Appendix  
## Research Context
This library bridges **Computational Mechanics** (the mathematical theory of complex systems and information processing) with modern **Neural Sequence Modeling**. It addresses the fundamental question: "How do neural networks learn to represent and predict the hidden structure in sequential data?"

## Key Research Questions
1. How well do different neural architectures learn the true computational structure of HMM-generated sequences?
2. What is the relationship between model capacity and ability to recover causal states?
3. How do different training procedures affect the quality of learned representations?
4. Can we develop better neural architectures based on computational mechanics principles?

## Technical Specifications
### Performance Requirements
- **Training Speed**: Models should train efficiently on GPU hardware
- **Memory Usage**: Support for sequences up to 10,000 timesteps
- **Accuracy**: Neural models should achieve >95% accuracy on simple HMM prediction tasks

### Compatibility Requirements  
- **Python**: 3.12+ for modern type system and performance
- **JAX**: Latest version for GPU acceleration and automatic differentiation
- **Hardware**: CUDA-compatible GPUs for training, CPU-only for inference

### Quality Requirements
- **Test Coverage**: >90% code coverage with comprehensive unit tests
- **Code Quality**: Strict linting with ruff, type checking with pyright
- **Documentation**: Full API documentation with examples and tutorials

## Related Work
- **Computational Mechanics**: Crutchfield & Young's epsilon-machine framework
- **Neural Sequence Modeling**: Modern transformer and RNN architectures  
- **Representation Learning**: Understanding what neural networks learn about sequential structure
- **Probabilistic Models**: Connections between neural and probabilistic sequence models
</PRD>