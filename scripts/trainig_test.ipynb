{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from epsilon_transformers.persistence import S3Persister, HackyPersister\n",
    "from epsilon_transformers.training.configs.model_configs import RawModelConfig\n",
    "from epsilon_transformers.process.processes import RRXOR, TransitionMatrixGHMM, ZeroOneR, Mess3\n",
    "from epsilon_transformers.analysis.activation_analysis import get_beliefs_for_transformer_inputs\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import pathlib\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.utils.data import IterableDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAN\n",
    "\n",
    "# 1. Generate all possible sequences of length seq_len + 1 and their probabilities, on CPU\n",
    "\n",
    "\n",
    "# 2. Create a dataset class that can draw samples from the process, all data should be on GPU\n",
    "# 3. Create a dataloader for the dataset\n",
    "# 4. Create a transformer model\n",
    "# 5. Train the model\n",
    "# 6. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epsilon_transformers.process.processes import Process\n",
    "from typing import Tuple\n",
    "\n",
    "def generate_all_seqs(process: Process, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate all possible sequences and their probabilities for a given process and sequence length.\n",
    "\n",
    "    Args:\n",
    "        process (Process): The process to generate sequences from.\n",
    "        seq_len (int): The length of sequences to generate.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "            - transformer_inputs: Tensor of shape (num_sequences, seq_len + 1) containing all possible sequences.\n",
    "            - probs: Tensor of shape (num_sequences,) containing the probability of each sequence.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the sum of probabilities is not equal to 1.0 (within floating-point precision).\n",
    "    \"\"\"\n",
    "    # Generate all paths and probabilities\n",
    "    msp = process.derive_mixed_state_presentation(depth=seq_len)\n",
    "    paths, probs = msp.get_paths_and_probs(depth=seq_len)\n",
    "\n",
    "    # Filter paths to keep only those of the desired length\n",
    "    valid_paths = [path for path in paths if len(path) == seq_len]\n",
    "    valid_probs = [prob for path, prob in zip(paths, probs) if len(path) == seq_len]\n",
    "\n",
    "    # Convert to tensors\n",
    "    transformer_inputs = torch.tensor(valid_paths, dtype=torch.int32)\n",
    "    probs = torch.tensor(valid_probs, dtype=torch.float32)\n",
    "\n",
    "    # Validate probabilities sum to 1\n",
    "    if not torch.allclose(probs.sum(), torch.tensor(1.0)):\n",
    "        error_message = f\"The sum of probabilities is not equal to 1.0. Actual sum: {probs.sum().item():.6f}\"\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "    return transformer_inputs, probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29faad3305946c09ffe6f9167d3b91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ad8616939747c5a72055b716ba9556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 1.1021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e23607ac0ac4d858c2ea84bf5640d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 1.0923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea822fcff0bb495fa12ffaadd645b260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 1.0908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe0309f1d10430281ebd7b9f3f5ac5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 1.0908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4170306e024499a9c821b0e878cbead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 1.0927\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050b99425f6548e5b272e593caab15c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Loss: 1.0914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378186306ea44ace8fa4d64ed7d64975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     83\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, Y \u001b[38;5;129;01min\u001b[39;00m tqdm(batch_generator, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     85\u001b[0m         model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     86\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/epsilon-machine/lib/python3.11/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/epsilon-machine/lib/python3.11/site-packages/tqdm/std.py:1192\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1190\u001b[0m dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m mininterval \u001b[38;5;129;01mand\u001b[39;00m cur_t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_start_t:\n\u001b[0;32m-> 1192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(n \u001b[38;5;241m-\u001b[39m last_print_n)\n\u001b[1;32m   1193\u001b[0m     last_print_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_n\n\u001b[1;32m   1194\u001b[0m     last_print_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_t\n",
      "File \u001b[0;32m~/anaconda3/envs/epsilon-machine/lib/python3.11/site-packages/tqdm/notebook.py:261\u001b[0m, in \u001b[0;36mtqdm_notebook.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39mn)\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;66;03m# cannot catch KeyboardInterrupt when using manual tqdm\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# as the interrupt will most likely happen on another statement\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/epsilon-machine/lib/python3.11/site-packages/tqdm/std.py:1243\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[0;32m-> 1243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh(lock_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_args)\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[0;32m~/anaconda3/envs/epsilon-machine/lib/python3.11/site-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1347\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n",
      "File \u001b[0;32m~/anaconda3/envs/epsilon-machine/lib/python3.11/site-packages/tqdm/std.py:104\u001b[0m, in \u001b[0;36mTqdmDefaultWriteLock.acquire\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocks:\n\u001b[0;32m--> 104\u001b[0m         lock\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer, HookedTransformerConfig # type: ignore\n",
    "from typing import Optional\n",
    "\n",
    "# Configuration\n",
    "cfg = {\n",
    "    \"seq_len\": 8,\n",
    "    \"batch_size\": 64,\n",
    "    \"batches_per_epoch\": 100,\n",
    "    \"device\": torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "# Process initialization\n",
    "process = TransitionMatrixGHMM(Mess3().transition_matrix)\n",
    "process.name = \"RRXOR\"\n",
    "\n",
    "# Generate sequences and probabilities\n",
    "transformer_inputs, probs = generate_all_seqs(process, cfg[\"seq_len\"] + 1)\n",
    "transformer_inputs = transformer_inputs.to(cfg[\"device\"])\n",
    "probs = probs.to(cfg[\"device\"])\n",
    "\n",
    "# Create an iterable batch generator\n",
    "class BatchGenerator:\n",
    "    def __init__(self, transformer_inputs, probs, cfg):\n",
    "        self.transformer_inputs = transformer_inputs\n",
    "        self.probs = probs\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cfg[\"batches_per_epoch\"]\n",
    "\n",
    "    def __iter__(self):\n",
    "        total_samples = self.cfg[\"batches_per_epoch\"] * self.cfg[\"batch_size\"]\n",
    "        sample_inds = torch.multinomial(self.probs, total_samples, replacement=True)\n",
    "        sample_inds = sample_inds.reshape(self.cfg[\"batches_per_epoch\"], self.cfg[\"batch_size\"])\n",
    "\n",
    "        for batch_indices in sample_inds:\n",
    "            batch = self.transformer_inputs[batch_indices]\n",
    "            X, Y = batch[:, :-1], batch[:, 1:]\n",
    "            yield X, Y\n",
    "\n",
    "# Create the batch generator\n",
    "batch_generator = BatchGenerator(transformer_inputs, probs, cfg)\n",
    "\n",
    "model_cfg = {\n",
    "    \"d_model\": 32,\n",
    "    \"d_head\": 8,\n",
    "    \"n_layers\": 1,\n",
    "    \"n_ctx\": cfg[\"seq_len\"] + 1,\n",
    "    \"n_heads\": 4,\n",
    "    \"attn_only\": False,\n",
    "    \"act_fn\": \"relu\",\n",
    "    \"positional_embedding_type\": \"standard\",\n",
    "    \"normalization_type\": \"LN\",\n",
    "}\n",
    "\n",
    "def create_hooked_transformer(model_cfg: dict, device: torch.device, seed: Optional[int] = None\n",
    "    ) -> HookedTransformer:\n",
    "        config = HookedTransformerConfig(\n",
    "            d_model=model_cfg[\"d_model\"],\n",
    "            d_head=model_cfg[\"d_head\"],\n",
    "            n_layers=model_cfg[\"n_layers\"],\n",
    "            n_ctx=model_cfg[\"n_ctx\"],\n",
    "            n_heads=model_cfg[\"n_heads\"],\n",
    "            d_mlp=4 * model_cfg[\"d_model\"],\n",
    "            d_vocab=3,\n",
    "            attn_only=model_cfg[\"attn_only\"],\n",
    "            seed=seed,\n",
    "            device=device,\n",
    "            act_fn=model_cfg[\"act_fn\"],\n",
    "            positional_embedding_type=model_cfg[\"positional_embedding_type\"],\n",
    "            normalization_type=model_cfg[\"normalization_type\"],\n",
    "        )\n",
    "        return HookedTransformer(config).to(device)\n",
    "\n",
    "model = create_hooked_transformer(model_cfg, cfg[\"device\"])\n",
    "from torch.nn.functional import cross_entropy\n",
    "# train!\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for n_epoch in tqdm(range(10), desc=\"Epochs\"):\n",
    "    epoch_loss = 0\n",
    "    for X, Y in tqdm(batch_generator, desc=\"Batches\", leave=False):\n",
    "        model.train()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(X)\n",
    "        logits = logits.reshape(-1, 3)\n",
    "        Y = Y.reshape(-1)\n",
    "        loss = cross_entropy(logits, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(batch_generator)\n",
    "    tqdm.write(f\"Epoch {n_epoch+1}, Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epsilon-machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
