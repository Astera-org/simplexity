name: transformer
instance:
  _target_: penzai.models.transformer.variants.llamalike_common.build_llamalike_transformer
  config:
    _target_: penzai.models.transformer.variants.llamalike_common.LlamalikeTransformerConfig
    num_kv_heads: 4
    query_head_multiplier: 2
    embedding_dim: 32
    projection_dim: 32
    mlp_hidden_dim: 32
    num_decoder_blocks: 4
    vocab_size: ${training_data_generator.vocab_size}
    mlp_variant: geglu_approx
    tie_embedder_and_logits: false
    rope_wavelength: 10000
    rms_norm_eps: 1e-06
    attention_type:
      _target_: penzai.models.transformer.variants.llamalike_common.AttentionTypeGlobalCausal
    use_post_attn_norm: false
    use_post_ffw_norm: false
    final_logit_softcap: null
    attn_logits_soft_cap: null
    query_scaling_factor: default
    parameter_dtype:
      _target_: jax.numpy.dtype
      _args_:
        - float32
    activation_dtype:
      _target_: jax.numpy.dtype
      _args_:
        - float32
    use_layer_stack: false
  init_base_rng:
    _target_: jax.random.PRNGKey
    seed: ${seed}
  name: transformer

load_checkpoint_step:
