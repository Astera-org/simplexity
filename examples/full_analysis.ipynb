{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from epsilon_transformers.process.GHMM import markov_approximation\n",
    "from epsilon_transformers.analysis.load_data import S3ModelLoader\n",
    "from epsilon_transformers.analysis.activation_analysis import (\n",
    "    prepare_msp_data,\n",
    "    run_activation_to_beliefs_regression,\n",
    "    get_sweep_type,\n",
    "    model_type,\n",
    "    get_activations,\n",
    "    plot_belief_prediction_comparison,\n",
    "    analyze_all_layers,\n",
    "    analyze_model_checkpoint,\n",
    "    markov_approx_msps,\n",
    "    shuffle_belief_norms,\n",
    "    save_nn_data\n",
    ")\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweeps to Analyze\n",
    "\n",
    "**RNN Experiments:**\n",
    "- 20241121152808\n",
    "    - 64 hidden state dims\n",
    "    - 1,2, and 4 layers\n",
    "    - [wandb](https://wandb.ai/adamimos/quantum_rnn_experiments_20241121152808)\n",
    "\n",
    "**Transformer Experiments:**\n",
    "- 20241205175736\n",
    "    - 64 residual stream dims\n",
    "    - 1,2, and 4 layers\n",
    "    - [wandb](https://wandb.ai/adamimos/quantum_transformer_20241205175736)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e1f99e97fc453f91aa57495641a92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_10_L1_H64_GRU_uni_tom_quantum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7162aaf1899642a48dcf19a35a063736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Markov Approximations:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m nn_shuffled_beliefs \u001b[38;5;241m=\u001b[39m shuffle_belief_norms(nn_unnormalized_beliefs)\n\u001b[1;32m     41\u001b[0m data_to_save[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffled_beliefs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m nn_shuffled_beliefs\n\u001b[0;32m---> 43\u001b[0m markov_data \u001b[38;5;241m=\u001b[39m markov_approx_msps(config, max_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m order, mark_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(markov_data):\n\u001b[1;32m     45\u001b[0m     mark_inputs, mark_beliefs, mark_indices, mark_probs, mark_unnorm \u001b[38;5;241m=\u001b[39m mark_data\n",
      "File \u001b[0;32m~/Documents/GitHub/epsilon-transformers/epsilon_transformers/analysis/activation_analysis.py:498\u001b[0m, in \u001b[0;36mmarkov_approx_msps\u001b[0;34m(run_config, max_order)\u001b[0m\n\u001b[1;32m    496\u001b[0m     markov_approx \u001b[38;5;241m=\u001b[39m markov_approximation(ghmm, order)\n\u001b[1;32m    497\u001b[0m     msp \u001b[38;5;241m=\u001b[39m markov_approx\u001b[38;5;241m.\u001b[39mderive_mixed_state_tree(depth\u001b[38;5;241m=\u001b[39mrun_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_ctx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 498\u001b[0m     markov_data\u001b[38;5;241m.\u001b[39mappend(prepare_data_from_msp(msp, run_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_ctx\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m markov_data\n",
      "File \u001b[0;32m~/Documents/GitHub/epsilon-transformers/epsilon_transformers/analysis/activation_analysis.py:105\u001b[0m, in \u001b[0;36mprepare_data_from_msp\u001b[0;34m(msp, path_length)\u001b[0m\n\u001b[1;32m    101\u001b[0m nn_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(nn_paths, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m probs_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mtuple\u001b[39m(path): prob \u001b[38;5;28;01mfor\u001b[39;00m path, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tree_paths, path_probs)}\n\u001b[0;32m--> 105\u001b[0m nn_beliefs, nn_belief_indices, nn_probs, nn_unnormalized_beliefs \u001b[38;5;241m=\u001b[39m get_beliefs_for_nn_inputs(\n\u001b[1;32m    106\u001b[0m     nn_inputs,\n\u001b[1;32m    107\u001b[0m     msp_belief_index,\n\u001b[1;32m    108\u001b[0m     tree_paths,\n\u001b[1;32m    109\u001b[0m     tree_beliefs,\n\u001b[1;32m    110\u001b[0m     tree_unnormalized_beliefs,\n\u001b[1;32m    111\u001b[0m     probs_dict\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nn_inputs, nn_beliefs, nn_belief_indices, nn_probs, nn_unnormalized_beliefs\n",
      "File \u001b[0;32m~/Documents/GitHub/epsilon-transformers/epsilon_transformers/analysis/activation_analysis.py:76\u001b[0m, in \u001b[0;36mget_beliefs_for_nn_inputs\u001b[0;34m(nn_inputs, msp_belief_index, tree_paths, tree_beliefs, tree_unnormalized_beliefs, probs_dict)\u001b[0m\n\u001b[1;32m     74\u001b[0m belief_state \u001b[38;5;241m=\u001b[39m path_belief_dict[input_substring]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     75\u001b[0m belief_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(belief_state, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m X_beliefs[i, j] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(belief_state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mnn_inputs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     77\u001b[0m X_belief_indices[i, j] \u001b[38;5;241m=\u001b[39m msp_belief_index[\u001b[38;5;28mtuple\u001b[39m(belief_state)]\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m probs_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loader = S3ModelLoader()\n",
    "sweeps = {\n",
    "    '20241121152808': 'RNN',\n",
    "    '20241205175736': 'Transformer'\n",
    "}\n",
    "\n",
    "for sweep_id, sweep_type in sweeps.items():\n",
    "        sweep_config = loader.load_sweep_config(sweep_id)\n",
    "        runs = loader.list_runs_in_sweep(sweep_id)\n",
    "\n",
    "        for run in tqdm(runs[1:3]):\n",
    "            print(run)\n",
    "            ckpts = loader.list_checkpoints(sweep_id, run)\n",
    "            \n",
    "            # Load initial model and prepare data\n",
    "            model, config = loader.load_checkpoint(\n",
    "                sweep_id=sweep_id,\n",
    "                run_id=run,\n",
    "                checkpoint_key=ckpts[-1],\n",
    "                device='cpu'\n",
    "            )\n",
    "\n",
    "            nn_type = model_type(model)\n",
    "            \n",
    "            nn_data = prepare_msp_data(config, config['model_config'])\n",
    "            (nn_inputs, nn_beliefs, nn_belief_indices, \n",
    "             nn_probs, nn_unnormalized_beliefs) = nn_data\n",
    "            \n",
    "\n",
    "            # Save the data in a dictionary format\n",
    "            data_to_save = {\n",
    "                'inputs': nn_inputs,\n",
    "                'beliefs': nn_beliefs,\n",
    "                'belief_indices': nn_belief_indices,\n",
    "                'probs': nn_probs,\n",
    "                'unnormalized_beliefs': nn_unnormalized_beliefs\n",
    "            }\n",
    "            # Create shuff\n",
    "            # led version of unnormalized beliefs\n",
    "            nn_shuffled_beliefs = shuffle_belief_norms(nn_unnormalized_beliefs)\n",
    "            data_to_save['shuffled_beliefs'] = nn_shuffled_beliefs\n",
    "\n",
    "            markov_data = markov_approx_msps(config, max_order=3)\n",
    "            for order, mark_data in enumerate(markov_data):\n",
    "                mark_inputs, mark_beliefs, mark_indices, mark_probs, mark_unnorm = mark_data\n",
    "                mark_shuffled = shuffle_belief_norms(mark_unnorm)\n",
    "                \n",
    "                data_to_save.update({\n",
    "                    f'markov_order_{order}_inputs': mark_inputs,\n",
    "                    f'markov_order_{order}_beliefs': mark_beliefs,\n",
    "                    f'markov_order_{order}_indices': mark_indices,\n",
    "                    f'markov_order_{order}_probs': mark_probs,\n",
    "                    f'markov_order_{order}_unnormalized': mark_unnorm,\n",
    "                    f'markov_order_{order}_shuffled': mark_shuffled\n",
    "                })\n",
    "\n",
    "            print(f'the size of the data to save is {sys.getsizeof(data_to_save)/1024**2} MB')\n",
    "\n",
    "            save_nn_data(loader, sweep_id, run, data_to_save)\n",
    "            # Analyze last two checkpoints\n",
    "            for ckpt in ckpts[-2:]:\n",
    "                model, config = loader.load_checkpoint(\n",
    "                    sweep_id=sweep_id,\n",
    "                    run_id=run,\n",
    "                    checkpoint_key=ckpt,\n",
    "                    device='cpu'\n",
    "                )\n",
    "                sweep_type = get_sweep_type(run)\n",
    "\n",
    "                # Analyze normalized beliefs\n",
    "                analyze_model_checkpoint(\n",
    "                    model, nn_inputs, nn_type, nn_beliefs, \n",
    "                    nn_belief_indices, nn_probs, sweep_type, run, title=\"Normalized Beliefs\",\n",
    "                    loader=loader,\n",
    "                    checkpoint_key=ckpt,\n",
    "                    sweep_id=sweep_id\n",
    "                )\n",
    "\n",
    "                # Analyze unnormalized beliefs\n",
    "                analyze_model_checkpoint(\n",
    "                    model, nn_inputs, nn_type, nn_unnormalized_beliefs, \n",
    "                    nn_belief_indices, nn_probs, sweep_type, run, title=\"Unnormalized Beliefs\",\n",
    "                    loader=loader,\n",
    "                    checkpoint_key=ckpt,\n",
    "                    sweep_id=sweep_id\n",
    "                )\n",
    "\n",
    "\n",
    "                # Analyze shuffled unnormalized beliefs\n",
    "                analyze_model_checkpoint(\n",
    "                    model, nn_inputs, nn_type, nn_shuffled_beliefs, \n",
    "                    nn_belief_indices, nn_probs, sweep_type, run, title=\"Shuffled Unnormalized Beliefs\",\n",
    "                    loader=loader,\n",
    "                    checkpoint_key=ckpt,\n",
    "                    sweep_id=sweep_id\n",
    "                )\n",
    "\n",
    "\n",
    "                # Analyze markov approximations\n",
    "                for order, mark_data in enumerate(markov_data):\n",
    "                    # unpack the data\n",
    "                    nn_inputs, nn_beliefs, nn_belief_indices, nn_probs, nn_unnormalized_beliefs = mark_data\n",
    "                     \n",
    "                    # Create shuffled version of unnormalized beliefs\n",
    "                    nn_shuffled_beliefs = shuffle_belief_norms(nn_unnormalized_beliefs)\n",
    "                    \n",
    "                    # Analyze normalized beliefs\n",
    "                    analyze_model_checkpoint(\n",
    "                        model, nn_inputs, nn_type, nn_beliefs, \n",
    "                        nn_belief_indices, nn_probs, sweep_type, run, title=f\"Order-{order} Approx.\",\n",
    "                        loader=loader,\n",
    "                        checkpoint_key=ckpt,\n",
    "                        sweep_id=sweep_id\n",
    "                    )\n",
    "                     \n",
    "                    # Analyze unnormalized beliefs\n",
    "                    analyze_model_checkpoint(\n",
    "                        model, nn_inputs, nn_type, nn_unnormalized_beliefs, \n",
    "                        nn_belief_indices, nn_probs, sweep_type, run, title=f\"Order-{order} Approx. Unnormalized\",\n",
    "                        loader=loader,\n",
    "                        checkpoint_key=ckpt,\n",
    "                        sweep_id=sweep_id\n",
    "                    )\n",
    "\n",
    "                    # Analyze shuffled unnormalized beliefs\n",
    "                    analyze_model_checkpoint(\n",
    "                        model, nn_inputs, nn_type, nn_shuffled_beliefs, \n",
    "                        nn_belief_indices, nn_probs, sweep_type, run, title=f\"Order-{order} Approx. Shuffled Unnormalized\",\n",
    "                        loader=loader,\n",
    "                        checkpoint_key=ckpt,\n",
    "                        sweep_id=sweep_id\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nn_unnormalized_beliefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epsilon-machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
