{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from epsilon_transformers.process.GHMM import markov_approximation\n",
    "from epsilon_transformers.analysis.load_data import S3ModelLoader\n",
    "from epsilon_transformers.analysis.activation_analysis import (\n",
    "    prepare_msp_data,\n",
    "    run_activation_to_beliefs_regression,\n",
    "    get_sweep_type,\n",
    "    model_type,\n",
    "    get_activations,\n",
    "    plot_belief_prediction_comparison,\n",
    "    analyze_all_layers,\n",
    "    analyze_model_checkpoint,\n",
    "    markov_approx_msps,\n",
    "    shuffle_belief_norms,\n",
    "    save_nn_data\n",
    ")\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweeps to Analyze\n",
    "\n",
    "**RNN Experiments:**\n",
    "- 20241121152808\n",
    "    - 64 hidden state dims\n",
    "    - 1,2, and 4 layers\n",
    "    - [wandb](https://wandb.ai/adamimos/quantum_rnn_experiments_20241121152808)\n",
    "\n",
    "**Transformer Experiments:**\n",
    "- 20241205175736\n",
    "    - 64 residual stream dims\n",
    "    - 1,2, and 4 layers\n",
    "    - [wandb](https://wandb.ai/adamimos/quantum_transformer_20241205175736)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e1f99e97fc453f91aa57495641a92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_10_L1_H64_GRU_uni_tom_quantum\n"
     ]
    }
   ],
   "source": [
    "loader = S3ModelLoader()\n",
    "sweeps = {\n",
    "    '20241121152808': 'RNN',\n",
    "    '20241205175736': 'Transformer'\n",
    "}\n",
    "\n",
    "for sweep_id, sweep_type in sweeps.items():\n",
    "        sweep_config = loader.load_sweep_config(sweep_id)\n",
    "        runs = loader.list_runs_in_sweep(sweep_id)\n",
    "\n",
    "        for run in tqdm(runs[1:3]):\n",
    "            print(run)\n",
    "            ckpts = loader.list_checkpoints(sweep_id, run)\n",
    "            \n",
    "            # Load initial model and prepare data\n",
    "            model, config = loader.load_checkpoint(\n",
    "                sweep_id=sweep_id,\n",
    "                run_id=run,\n",
    "                checkpoint_key=ckpts[-1],\n",
    "                device='cpu'\n",
    "            )\n",
    "\n",
    "            nn_type = model_type(model)\n",
    "            \n",
    "            nn_data = prepare_msp_data(config, config['model_config'])\n",
    "            (nn_inputs, nn_beliefs, nn_belief_indices, \n",
    "             nn_probs, nn_unnormalized_beliefs) = nn_data\n",
    "            \n",
    "\n",
    "            # Save the data in a dictionary format\n",
    "            data_to_save = {\n",
    "                'inputs': nn_inputs,\n",
    "                'beliefs': nn_beliefs,\n",
    "                'belief_indices': nn_belief_indices,\n",
    "                'probs': nn_probs,\n",
    "                'unnormalized_beliefs': nn_unnormalized_beliefs\n",
    "            }\n",
    "            # Create shuff\n",
    "            # led version of unnormalized beliefs\n",
    "            nn_shuffled_beliefs = shuffle_belief_norms(nn_unnormalized_beliefs)\n",
    "            data_to_save['shuffled_beliefs'] = nn_shuffled_beliefs\n",
    "\n",
    "            markov_data = markov_approx_msps(config, max_order=3)\n",
    "            for order, mark_data in enumerate(markov_data):\n",
    "                mark_inputs, mark_beliefs, mark_indices, mark_probs, mark_unnorm = mark_data\n",
    "                mark_shuffled = shuffle_belief_norms(mark_unnorm)\n",
    "                \n",
    "                data_to_save.update({\n",
    "                    f'markov_order_{order}_inputs': mark_inputs,\n",
    "                    f'markov_order_{order}_beliefs': mark_beliefs,\n",
    "                    f'markov_order_{order}_indices': mark_indices,\n",
    "                    f'markov_order_{order}_probs': mark_probs,\n",
    "                    f'markov_order_{order}_unnormalized': mark_unnorm,\n",
    "                    f'markov_order_{order}_shuffled': mark_shuffled\n",
    "                })\n",
    "\n",
    "            print(f'the size of the data to save is {sys.getsizeof(data_to_save)/1024**2} MB')\n",
    "\n",
    "            save_nn_data(loader, sweep_id, run, data_to_save)\n",
    "            # Analyze last two checkpoints\n",
    "            for ckpt in ckpts[-2:]:\n",
    "                model, config = loader.load_checkpoint(\n",
    "                    sweep_id=sweep_id,\n",
    "                    run_id=run,\n",
    "                    checkpoint_key=ckpt,\n",
    "                    device='cpu'\n",
    "                )\n",
    "                sweep_type = get_sweep_type(run)\n",
    "\n",
    "                # Analyze normalized beliefs\n",
    "                analyze_model_checkpoint(\n",
    "                    model, nn_inputs, nn_type, nn_beliefs, \n",
    "                    nn_belief_indices, nn_probs, sweep_type, run, title=\"Normalized Beliefs\",\n",
    "                    loader=loader,\n",
    "                    checkpoint_key=ckpt,\n",
    "                    sweep_id=sweep_id\n",
    "                )\n",
    "\n",
    "                # Analyze unnormalized beliefs\n",
    "                analyze_model_checkpoint(\n",
    "                    model, nn_inputs, nn_type, nn_unnormalized_beliefs, \n",
    "                    nn_belief_indices, nn_probs, sweep_type, run, title=\"Unnormalized Beliefs\",\n",
    "                    loader=loader,\n",
    "                    checkpoint_key=ckpt,\n",
    "                    sweep_id=sweep_id\n",
    "                )\n",
    "\n",
    "\n",
    "                # Analyze shuffled unnormalized beliefs\n",
    "                analyze_model_checkpoint(\n",
    "                    model, nn_inputs, nn_type, nn_shuffled_beliefs, \n",
    "                    nn_belief_indices, nn_probs, sweep_type, run, title=\"Shuffled Unnormalized Beliefs\",\n",
    "                    loader=loader,\n",
    "                    checkpoint_key=ckpt,\n",
    "                    sweep_id=sweep_id\n",
    "                )\n",
    "\n",
    "\n",
    "                # Analyze markov approximations\n",
    "                for order, mark_data in enumerate(markov_data):\n",
    "                    # unpack the data\n",
    "                    nn_inputs, nn_beliefs, nn_belief_indices, nn_probs, nn_unnormalized_beliefs = mark_data\n",
    "                     \n",
    "                    # Create shuffled version of unnormalized beliefs\n",
    "                    nn_shuffled_beliefs = shuffle_belief_norms(nn_unnormalized_beliefs)\n",
    "                    \n",
    "                    # Analyze normalized beliefs\n",
    "                    analyze_model_checkpoint(\n",
    "                        model, nn_inputs, nn_type, nn_beliefs, \n",
    "                        nn_belief_indices, nn_probs, sweep_type, run, title=f\"Order-{order} Approx.\",\n",
    "                        loader=loader,\n",
    "                        checkpoint_key=ckpt,\n",
    "                        sweep_id=sweep_id\n",
    "                    )\n",
    "                     \n",
    "                    # Analyze unnormalized beliefs\n",
    "                    analyze_model_checkpoint(\n",
    "                        model, nn_inputs, nn_type, nn_unnormalized_beliefs, \n",
    "                        nn_belief_indices, nn_probs, sweep_type, run, title=f\"Order-{order} Approx. Unnormalized\",\n",
    "                        loader=loader,\n",
    "                        checkpoint_key=ckpt,\n",
    "                        sweep_id=sweep_id\n",
    "                    )\n",
    "\n",
    "                    # Analyze shuffled unnormalized beliefs\n",
    "                    analyze_model_checkpoint(\n",
    "                        model, nn_inputs, nn_type, nn_shuffled_beliefs, \n",
    "                        nn_belief_indices, nn_probs, sweep_type, run, title=f\"Order-{order} Approx. Shuffled Unnormalized\",\n",
    "                        loader=loader,\n",
    "                        checkpoint_key=ckpt,\n",
    "                        sweep_id=sweep_id\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nn_unnormalized_beliefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epsilon-machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
