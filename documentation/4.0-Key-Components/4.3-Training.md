# 4.3. Training

The `simplexity.training` module contains the core logic for training the predictive models. The primary script used for models built with Equinox is `train_equinox_model.py`.

## Main Training Function: `train()`

The central function is `train(...)` located in `simplexity.training.train_equinox_model.py`. It orchestrates the entire training process.

**Signature:**
```python
def train(
    model: PredictiveModel,
    training_cfg: TrainingConfig, # From simplexity.configs.training.config
    training_data_generator: GenerativeProcess,
    logger: Logger | None = None,
    validation_cfg: ValidationConfig | None = None, # From simplexity.configs.evaluation.config
    validation_data_generator: GenerativeProcess | None = None,
    persister: ModelPersister | None = None,
) -> tuple[PredictiveModel, float]:
    # ... implementation ...
```

**Key Operations:**

1.  **Initialization:**
    *   Sets up a JAX PRNG key using `training_cfg.seed`.
    *   Instantiates the optimizer (e.g., Adam, AdamW) using `simplexity.utils.hydra.typed_instantiate`. The optimizer type and its hyperparameters (like learning rate, weight decay) are specified in the `training_cfg.optimizer.instance` part of the Hydra configuration.
        *   Example optimizer config (`simplexity/configs/optimizer/adamw.yaml`):
            ```yaml
            _target_: optax.adamw
            learning_rate: 0.001
            # ... other adamw params
            ```
    *   Initializes the optimizer state using the model's initial parameters.
    *   Retrieves the initial state from the `training_data_generator` and replicates it across the batch dimension (`training_cfg.batch_size`).

2.  **Training Loop:**
    *   The loop runs for `training_cfg.num_steps` iterations.
    *   In each step:
        1.  **Data Generation (`generate_data_batch`):**
            *   A fresh batch of sequences is generated using the `training_data_generator`.
            *   Inputs are typically one-hot encoded sequences (shifted one step), and labels are the target next tokens.
        2.  **Training Step (`training_step`):**
            *   **Loss Calculation (`loss_fn`):** The current `model` processes the `inputs` to produce `logits`. The loss (e.g., softmax cross-entropy) is calculated between these `logits` and the `labels`. This function is also responsible for computing gradients with respect to the model parameters (`eqx.filter_value_and_grad`).
            *   **Model Update (`update_model`):** Gradients are averaged across the batch. The optimizer uses these gradients to update the model's parameters (`eqx.apply_updates`).
        3.  **Logging (Optional):** If a `logger` instance is provided, training metrics (like loss) are logged every `training_cfg.log_every` steps.
        4.  **Validation (Optional):** If `validation_cfg` and `validation_data_generator` are provided, the model is evaluated on a validation data batch every `training_cfg.validate_every` steps. The evaluation is performed by `simplexity.evaluation.evaluate_equinox_model.evaluate`, and resulting validation metrics are logged.
        5.  **Checkpointing (Optional):** If a `persister` instance is provided, the current state of the `model` is saved to disk (or other storage) every `training_cfg.checkpoint_every` steps.

3.  **Return Value:** The function returns the trained `model` and the final training `loss` value.

**JAX/Equinox Features:**
*   Key functions like `generate_data_batch`, `loss_fn`, `update_model`, and `training_step` are JIT-compiled using `@eqx.filter_jit` for optimal performance on accelerators (GPU/TPU).
*   `loss_fn` uses `@eqx.filter_vmap` for batch processing and `@eqx.filter_value_and_grad` for efficient gradient computation.
*   Model parameters and optimizer state are handled as PyTrees, fitting naturally with JAX and Equinox's functional programming paradigm.

## Configuration

Training behavior is controlled by the `TrainingConfig` dataclass, populated from YAML files typically found in `simplexity/configs/train/` (e.g., `medium.yaml`).

Key configurable parameters include:
*   `seed`: Random seed for reproducibility.
*   `optimizer`: A nested configuration specifying the optimizer type (via `_target_`) and its parameters (e.g., `learning_rate`).
*   `batch_size`: Number of sequences per training batch.
*   `sequence_len`: Length of the sequences generated for training.
*   `num_steps`: Total number of training iterations.
*   `log_every`: Frequency of logging training metrics.
*   `validate_every`: Frequency of running validation.
*   `checkpoint_every`: Frequency of saving model checkpoints.

Example snippet from a training configuration (`simplexity/configs/train/medium.yaml`):
```yaml
_target_: simplexity.configs.training.config.Config
seed: 0
batch_size: 64
sequence_len: 128
num_steps: 10000
log_every: 100
validate_every: 1000
checkpoint_every: 1000

optimizer:
  _target_: simplexity.configs.optimizer.config.Config
  name: adamw
  instance:
    _target_: optax.adamw
    learning_rate: 0.001
    # ... other adamw parameters
```

## For Experimentalists

*   Adjust training duration (`num_steps`), batch properties (`batch_size`, `sequence_len`), logging/validation/checkpointing frequencies by modifying the chosen training configuration YAML (e.g., `medium.yaml`) or by overriding parameters from the command line:
    ```bash
    python simplexity/run_experiment.py train.num_steps=20000 train.optimizer.instance.learning_rate=0.0005
    ```
*   Experiment with different optimizers by changing the `train.optimizer.name` and ensuring a corresponding config file exists in `simplexity/configs/optimizer/` or by directly setting `train.optimizer.instance._target_` and its parameters.

## For LLM Agents

*   Agents can control the entire training process by modifying the `training` section of the Hydra configuration.
*   This includes selecting optimizers, setting learning rates, batch sizes, training duration, and controlling how often logging, validation, and checkpointing occur.
*   The path to training parameters is typically `train.<parameter_name>` (e.g., `train.num_steps`) or `train.optimizer.instance.<optimizer_param>` (e.g., `train.optimizer.instance.learning_rate`). 