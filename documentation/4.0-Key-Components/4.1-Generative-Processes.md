# 4.1. Generative Processes

Generative Processes are responsible for producing the sequential data used for training and evaluating predictive models. In this framework, these are primarily implemented as Hidden Markov Models (HMMs) and Generalized Hidden Markov Models (GHMMs).

## Core Interface: `GenerativeProcess`

All generative processes adhere to the interface defined by the abstract base class `simplexity.generative_processes.generative_process.GenerativeProcess`. This class is an `equinox.Module` and defines the following key methods and properties that any specific generative process must implement:

*   **`vocab_size` (property):** An integer representing the number of unique symbols or observations the process can emit.
*   **`initial_state` (property):** The starting state (or distribution over states) of the process.
*   **`emit_observation(state: State, key: chex.PRNGKey) -> chex.Array`:** Given the current hidden state, this method stochastically emits an observation.
*   **`transition_states(state: State, obs: chex.Array) -> State`:** Given the current hidden state and an observation, this method transitions the process to a new hidden state.
*   **`generate(state: State, key: chex.PRNGKey, sequence_len: int, return_all_states: bool) -> tuple[State, chex.Array]`:** This is a crucial method used by the framework to generate batches of data sequences of a specified length.
*   **`observation_probability_distribution(state: State) -> jax.Array`:** Returns the probability distribution over the vocabulary given a hidden state.
*   **`log_observation_probability_distribution(log_belief_state: State) -> jax.Array`:** Similar to the above, but works with log probabilities and log belief states.
*   **`probability(observations: jax.Array) -> jax.Array`:** Computes the probability of the process generating a given sequence of observations.
*   **`log_probability(observations: jax.Array) -> jax.Array`:** Computes the log probability of a sequence.

## Implementations: HMMs and GHMMs

The primary implementations are:
*   `simplexity.generative_processes.hidden_markov_model.HiddenMarkovModel`
*   `simplexity.generative_processes.generalized_hidden_markov_model.GeneralizedHiddenMarkovModel`

These models are defined by their transition dynamics, which are encapsulated in transition matrices.

## Configuration and Building

Generative processes are configured via YAML files in the `simplexity/configs/generative_process/` directory (e.g., `mess3.yaml`, `even_ones.yaml`).

A typical configuration for a generative process looks like this (e.g., from `mess3.yaml` which might be simplified here for illustration):

```yaml
# In simplexity/configs/generative_process/mess3.yaml
_target_: simplexity.generative_processes.builder.build_hidden_markov_model
process_name: mess3
# ... other parameters specific to "mess3", like p, q, etc.
```

Key points:
*   `_target_`: This Hydra key points to a builder function in `simplexity.generative_processes.builder.py` (e.g., `build_hidden_markov_model` or `build_generalized_hidden_markov_model`).
*   `process_name`: This string (e.g., "mess3") is used by the builder function to look up a specific function that constructs the transition matrices for that named process. These matrix-generating functions are found in `simplexity.generative_processes.transition_matrices.py` (within `HMM_MATRIX_FUNCTIONS` or `GHMM_MATRIX_FUNCTIONS` dictionaries).
*   Other parameters (e.g., `p` or `alpha` depending on the process) are passed as keyword arguments to the matrix-generating function for the specified `process_name`.

The builder function then instantiates the appropriate HMM or GHMM class with these dynamically generated transition matrices.

## How Data is Generated for Training

When an experiment run starts (e.g., via `run_experiment.py`):
1.  Hydra reads the configuration for `training_data_generator` and `validation_data_generator`.
2.  It calls the specified `_target_` builder function with the parameters from the YAML file.
3.  This creates instances of `HiddenMarkovModel` or `GeneralizedHiddenMarkovModel`.
4.  During the training loop, the `generate()` method of these instances is called repeatedly with appropriate JAX PRNG keys to produce batches of training and validation sequences.

## For Experimentalists

*   To use a different generative process for your experiment, you modify the `defaults` list in your main `experiment.yaml`:
    ```yaml
    defaults:
      # ...
      - generative_process@training_data_generator: <new_process_name>
      - generative_process@validation_data_generator: <new_process_name>
      # ...
    ```
    Where `<new_process_name>` is the name of a YAML file in `simplexity/configs/generative_process/` (e.g., `even_ones`).
*   To change parameters of a generative process (e.g., the `p` value for `even_ones`), you can either:
    *   Edit the corresponding YAML file (e.g., `simplexity/configs/generative_process/even_ones.yaml`).
    *   Override it from the command line: `python simplexity/run_experiment.py training_data_generator.p=0.7` (assuming `p` is a direct parameter under the `even_ones` config).

## For LLM Agents

*   Agents can select or configure generative processes by modifying the `training_data_generator` and `validation_data_generator` sections of the main experiment configuration or by directly creating/editing the YAML files in `simplexity/configs/generative_process/`.
*   Understanding the `_target_` and `process_name` mechanism is key to programmatically selecting different data sources.
*   The parameters available for each `process_name` can be inferred by looking at the signatures of the functions within `HMM_MATRIX_FUNCTIONS` and `GHMM_MATRIX_FUNCTIONS` in `simplexity.generative_processes.transition_matrices.py` or by examining existing YAML configuration files for those processes. 