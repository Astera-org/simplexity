# 4.6. Evaluation

The `simplexity.evaluation` module is responsible for assessing the performance of trained predictive models. This typically involves calculating metrics like loss and accuracy on a validation or test dataset generated by a `GenerativeProcess`.

## Main Evaluation Function: `evaluate()`

The primary function for evaluating Equinox-based models is `evaluate(...)` in `simplexity.evaluation.evaluate_equinox_model.py`.

**Signature:**
```python
def evaluate(
    model: PredictiveModel,
    cfg: Config,  # simplexity.configs.evaluation.config.Config
    data_generator: GenerativeProcess,
    logger: Logger | None = None,
) -> dict[str, jax.Array]:
    # ... implementation ...
```

**Key Operations:**

1.  **Initialization:**
    *   Sets up a JAX PRNG key using `cfg.seed`.
    *   Retrieves the initial state from the `data_generator` and replicates it across the batch dimension (`cfg.batch_size`).
    *   Initializes a dictionary to accumulate metrics.

2.  **Evaluation Loop:**
    *   The loop runs for `cfg.num_steps` iterations (number of evaluation batches).
    *   In each step:
        1.  **Data Generation (`generate_data_batch`):**
            *   A fresh batch of sequences is generated using the `data_generator` with `cfg.batch_size` and `cfg.sequence_len`.
            *   Inputs (one-hot encoded, shifted) and labels (target next tokens) are prepared, identical to the training data generation.
        2.  **Evaluation Step (`evaluation_step`):**
            *   The `model` processes the `inputs` to produce `logits`.
            *   **Metric Calculation:** Metrics are computed using functions from `simplexity.evaluation.metric_functions.py`:
                *   `loss_fn(logits, labels)`: Calculates token-level softmax cross-entropy loss (same as in training).
                *   `accuracy_fn(logits, labels)`: Calculates token-level accuracy by comparing `argmax(logits)` with `labels`.
            *   The `evaluation_step` returns a dictionary of metrics for the current batch (e.g., `{"loss": mean_batch_loss, "accuracy": mean_batch_accuracy}`).
        3.  **Metric Accumulation:** The metrics from the current batch are averaged and added to the cumulative metrics for the entire evaluation run.
        4.  **Logging (Optional):** If a `logger` is provided, the accumulated metrics can be logged periodically within the evaluation loop (controlled by `cfg.log_every`).

3.  **Return Value:** The function returns a dictionary containing the final mean metrics (e.g., average loss, average accuracy) computed over all `cfg.num_steps` evaluation batches.

**JAX/Equinox Features:**
*   `generate_data_batch` and `evaluation_step` are JIT-compiled with `@eqx.filter_jit`.
*   `evaluation_step` is also vmapped using `@eqx.filter_vmap` to process batches efficiently.

## Metric Functions (`metric_functions.py`)

This file defines the individual functions used to calculate metrics:
*   **`MetricFunction(Protocol)`:** A protocol defining the expected signature for any metric function (`__call__(self, logits: jax.Array, labels: jax.Array) -> jax.Array`).
*   **`loss_fn(logits: jax.Array, labels: jax.Array) -> jax.Array`:**
    *   Computes token-wise softmax cross-entropy loss using `optax.softmax_cross_entropy_with_integer_labels`.
*   **`accuracy_fn(logits: jax.Array, labels: jax.Array) -> jax.Array`:**
    *   Determines predicted tokens by `jnp.argmax(logits)`.
    *   Returns a boolean array indicating whether each prediction matches the corresponding label.
The `evaluation_step` function then typically computes the mean of these token-wise metrics over the sequence and batch dimensions.

## Configuration

Evaluation behavior is controlled by the `EvaluationConfig` (often aliased as `ValidationConfig` when used during training), populated from YAML files (e.g., `simplexity/configs/evaluation/default.yaml` or a specific section within a broader training config like `experiment.yaml`'s `validation` block).

Key configurable parameters (within `cfg` passed to `evaluate`):
*   `seed`: Random seed for reproducibility of evaluation data generation.
*   `batch_size`: Number of sequences per evaluation batch.
*   `sequence_len`: Length of sequences for evaluation.
*   `num_steps`: Number of batches to evaluate over.
*   `log_every`: Frequency of logging metrics *during* the evaluation loop (if a logger is passed directly to `evaluate`).

Example snippet for an evaluation configuration (which would map to `ValidationConfig`):
```yaml
# Part of experiment.yaml or a dedicated evaluation.yaml
_target_: simplexity.configs.evaluation.config.Config # Or ValidationConfig
seed: 123
batch_size: 32
sequence_len: 128
num_steps: 100 # Number of batches for validation
log_every: 10 # Log every 10 validation batches (if logger used directly)
```

## Usage During Training (Validation)

As seen in `simplexity.training.train_equinox_model.train`, the `evaluate` function is called periodically during the training loop if `validation_cfg` and `validation_data_generator` are provided. The metrics returned by `evaluate` are then logged, typically prefixed with "validation/" (e.g., "validation/loss", "validation/accuracy").

## For Experimentalists

*   Validation settings are part of the overall experiment configuration, usually under a `validation` key that maps to `ValidationConfig` (which is the same as `EvaluationConfig`).
*   You can adjust `num_steps` for validation to control how many batches are used to estimate validation performance, `batch_size`, etc., by editing the relevant YAML section or using command-line overrides (e.g., `python simplexity/run_experiment.py validation.num_steps=50`).

## For LLM Agents

*   Agents can configure how validation is performed by modifying the `validation` section (which maps to `ValidationConfig`/`EvaluationConfig`) in the main Hydra configuration.
*   This includes setting the number of evaluation steps (`num_steps`), batch size, and sequence length for the data used in validation.
*   The primary output an agent would be interested in are the metrics returned by the `evaluate` function, which are subsequently logged by the training loop. 