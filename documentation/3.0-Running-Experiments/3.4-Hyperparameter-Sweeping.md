# 3.4. Hyperparameter Sweeping (Optuna)

The Simplexity framework uses Hydra's plugin system to integrate with [Optuna](https://optuna.org/) for efficient hyperparameter optimization (HPO), also known as sweeping.

## How it Works

1.  **Configuration in `experiment.yaml`:**
    As detailed in the `experiment.yaml` documentation, hyperparameter sweeping is configured within the `hydra.sweeper` block. Key elements include:
    *   **Declaring Optuna as the Sweeper:**
        ```yaml
        defaults:
          # ... other defaults ...
          - override hydra/sweeper: optuna
        ```
    *   **Sweeper Settings:**
        ```yaml
        hydra:
          sweeper:
            direction: minimize  # or "maximize"
            n_trials: 20         # Total experiments to run
            study_name: my_hpo_study
            sampler:
              _target_: optuna.samplers.TPESampler # Or other Optuna samplers
              seed: ${seed}
              n_startup_trials: 10
        ```
    *   **Defining Search Spaces (`params`):**
        ```yaml
        hydra:
          sweeper:
            # ... other sweeper settings ...
            params:
              train.optimizer.instance.learning_rate: tag(log, interval(1e-4, 1e-1))
              train.batch_size: choice(16, 32, 64, 128)
              predictive_model.instance.hidden_size: int(interval(32, 256))
              # ... other parameters ...
        ```
        The syntax for defining search spaces (`interval`, `choice`, `tag(log, ...)`) comes from Hydra's Optuna integration. Refer to the [Hydra Optuna Sweeper plugin documentation](https://hydra.cc/docs/plugins/optuna_sweeper/) for full details on the available distributions and syntax.

2.  **Launching the Sweep:**
    When you run `python simplexity/run_experiment.py` with such a configuration, Hydra takes over:
    *   It recognizes the `--multirun` mode is implicitly activated by the sweeper configuration (or you can explicitly add `--multirun`).
    *   The Optuna sweeper plugin then interacts with Optuna's library.
    *   For each trial (up to `n_trials`):
        1.  Optuna's sampler (e.g., `TPESampler`) suggests a new set of hyperparameter values based on the defined search space and the results of previous trials.
        2.  Hydra launches a new run of `run_experiment.py` with these suggested hyperparameters (as if they were passed via command-line overrides).
        3.  The script `run_experiment.py` executes and returns its primary metric (e.g., final validation loss), which Hydra passes back to Optuna.
        4.  Optuna records this trial's parameters and result, using this information to guide future suggestions.

3.  **Output of a Sweep:**
    *   Each trial in a sweep will have its own output directory under `multirun/YYYY-MM-DD/HH-MM-SS-ms/<trial_id>/` (the exact path format can be configured).
    *   Each trial directory contains the usual `.hydra` subdirectory with the specific configuration for that trial, plus any logs and artifacts produced by that run.
    *   A summary of the sweep, often including the best parameters found, might be printed to the console by Hydra or logged by Optuna, depending on the configuration.
    *   If using a persistent Optuna `storage` (e.g., a database URL instead of `null`), the study results can be inspected later using Optuna's tools.

## Benefits of Using Optuna via Hydra

*   **Sophisticated Sampling:** Optuna provides advanced sampling algorithms (like TPE) that are more efficient than random or grid search for finding good hyperparameters.
*   **Pruning (Optional):** Optuna supports pruning unpromising trials early, saving computational resources. (This would require some integration within the training script to report intermediate values to Optuna).
*   **Parallelization:** Sweeps can be parallelized by setting `n_jobs` in the sweeper config or by using Optuna's distributed optimization features with a shared storage backend.
*   **Ease of Configuration:** Defining search spaces directly in the YAML configuration is convenient.

## For Experimentalists

*   **To start a sweep:** Ensure your `experiment.yaml` is configured for sweeping (as shown above) and simply run `python simplexity/run_experiment.py --multirun` (or often, just `python simplexity/run_experiment.py` if `n_trials > 1` in the sweeper config).
*   **To monitor:** Check the console output and the `multirun` directory. If MLFlow or another experiment tracker is configured, results for each trial will likely appear there.
*   **To customize:**
    *   Change `n_trials` to run more or fewer experiments.
    *   Adjust the parameter ranges or choices in `hydra.sweeper.params`.
    *   Try different Optuna samplers by changing `hydra.sweeper.sampler._target_`.

## For LLM Agents

*   **Modifying Sweeps:** An agent can easily modify the `hydra.sweeper.params` section of `experiment.yaml` to set up new HPO experiments based on user requests (e.g., "sweep learning rates between X and Y, and try batch sizes A, B, C").
*   **Launching and Monitoring:** The agent can launch sweeps using the standard command and then monitor the `multirun` directory or query the configured Optuna storage (if used) to find the best trial and its parameters.

Hyperparameter sweeping is a critical part of the machine learning workflow, and the integration with Optuna makes it a powerful feature of this framework. 