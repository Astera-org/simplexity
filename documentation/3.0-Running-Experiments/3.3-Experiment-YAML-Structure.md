# 3.3. Understanding `experiment.yaml`

The file `simplexity/configs/experiment.yaml` is a central configuration file for defining and running experiments. It leverages Hydra's features for composition and hyperparameter sweeping.

Let's break down a typical `experiment.yaml` structure (based on the one observed earlier):

```yaml
defaults:
  - _self_  # Includes any top-level keys defined directly in this file
  - generative_process@training_data_generator: mess3
  - generative_process@validation_data_generator: mess3
  - predictive_model: gru_rnn
  - persistence: local_persister
  - logging: mlflow_logger
  - train: medium
  - override hydra/sweeper: optuna # Overrides the default Hydra sweeper to use Optuna

seed: 0
experiment_name: ${predictive_model.name}_${training_data_generator.name}
run_name: ${now:%Y-%m-%d_%H-%M-%S}_${experiment_name}_${seed}

hydra:
  sweeper:
    # Optuna Sweeper specific configurations
    direction: minimize       # Direction of optimization (e.g., minimize loss)
    n_trials: 20              # Number of trials (experiments) to run for the sweep
    n_jobs: 1                 # Number of parallel jobs for sweeping (can be >1 if resources allow)
    storage: null             # Optuna storage URL (e.g., for distributed studies, null for in-memory)
    study_name: test          # Name of the Optuna study
    sampler:
      _target_: optuna.samplers.TPESampler  # Specifies the Optuna sampling algorithm
      seed: ${seed}                       # Seed for the sampler for reproducibility
      n_startup_trials: 10               # Number of initial random trials before sampler takes over

    # Parameters to sweep over
    params:
      train.optimizer.instance.learning_rate: tag(log, interval(1e-4, 1e-1))
      train.batch_size: choice(16, 32, 64, 128)
      
      predictive_model.instance.hidden_size: int(interval(32, 256))
      predictive_model.instance.num_layers: int(interval(1, 4))
```

## Key Sections Explained

1.  **`defaults` List:**
    *   This is the core of Hydra's composition mechanism.
    *   `- _self_`: Ensures that any keys defined directly in `experiment.yaml` (like `seed`, `experiment_name`, `hydra`) are included in the final configuration.
    *   `- generative_process@training_data_generator: mess3`:
        *   `generative_process`: This is the **config group** name. It tells Hydra to look into the `simplexity/configs/generative_process/` directory.
        *   `@training_data_generator`: This assigns the chosen configuration to the `training_data_generator` key in the main `Config` dataclass.
        *   `mess3`: This is the **config name**. Hydra will load `simplexity/configs/generative_process/mess3.yaml`.
    *   Similarly, it loads configurations for `validation_data_generator`, `predictive_model`, `persistence`, `logging`, and `train` from their respective config groups and files.
    *   `- override hydra/sweeper: optuna`:
        *   This is a special Hydra directive to override a default provided by Hydra itself.
        *   It replaces the default basic sweeper with the `optuna` sweeper, whose configuration is then defined under the `hydra.sweeper` key.

2.  **Top-Level Parameters:**
    *   `seed: 0`: A global random seed for the experiment, often used by various components for reproducibility.
    *   `experiment_name: ${predictive_model.name}_${training_data_generator.name}`:
        *   Defines a name for the experiment.
        *   It uses **variable interpolation** (`${...}`). Hydra will resolve `predictive_model.name` and `training_data_generator.name` from their respective loaded configurations to form the final name (e.g., `gru_rnn_mess3`).
    *   `run_name: ${now:%Y-%m-%d_%H-%M-%S}_${experiment_name}_${seed}`:
        *   Defines a unique name for each individual run, incorporating the current timestamp (`${now:...}` is a Hydra resolver), the `experiment_name`, and the `seed`.
        *   This is very useful for organizing outputs, especially with MLFlow or other tracking tools.

3.  **`hydra` Configuration Block:**
    *   This block is specifically for configuring Hydra's behavior and its plugins (like sweepers).
    *   **`hydra.sweeper`:** Contains settings for the Optuna sweeper.
        *   `direction`: `minimize` or `maximize`, depending on the metric being optimized.
        *   `n_trials`: Total number of different hyperparameter combinations to try.
        *   `sampler`: Configures the Optuna sampling algorithm (e.g., `TPESampler` for Tree-structured Parzen Estimator).
        *   **`params`:** This is where you define the hyperparameter search space.
            *   `train.optimizer.instance.learning_rate: tag(log, interval(1e-4, 1e-1))`: Defines that the learning rate should be sampled from a log-uniform distribution between 1e-4 and 1e-1. The `tag(log, ...)` is specific syntax for some sweepers/plugins to indicate the scale.
            *   `train.batch_size: choice(16, 32, 64, 128)`: Samples batch size from the given discrete choices.
            *   `predictive_model.instance.hidden_size: int(interval(32, 256))`: Samples an integer hidden size uniformly between 32 and 256.

## How to Use and Modify

*   **Change Default Components:** To use a different predictive model, say `my_transformer.yaml` (which you would place in `simplexity/configs/predictive_model/`), you would change the line in the `defaults` list to:
    ```yaml
    - predictive_model: my_transformer
    ```
*   **Modify Hyperparameters for a Single Run:** For a single run without sweeping, you would typically comment out or remove the `hydra.sweeper.params` section and rely on the default values set in the component configs (e.g., in `simplexity/configs/train/medium.yaml` or `simplexity/configs/predictive_model/gru_rnn.yaml`). Alternatively, use command-line overrides.
*   **Adjust Sweep Range:** Modify the `interval`, `choice`, or other distributions under `hydra.sweeper.params` to change the hyperparameter search space.
*   **Turn Off Sweeping:** To run with the specific default parameters listed and not perform a sweep, you can either remove the `hydra.sweeper` section or, more simply, comment out the `- override hydra/sweeper: optuna` line and the `hydra.sweeper.params` section. Alternatively, you can often just run `python simplexity/run_experiment.py` and it will use the default values from the composed configs if no explicit sweep instruction is active or `n_trials` is 1.

This file is the main control panel for defining what your experiment looks like and how hyperparameter searches are conducted. 