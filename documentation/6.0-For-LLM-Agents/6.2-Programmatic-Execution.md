# 6.2. Programmatic Experiment Execution

While LLM agents often modify configuration files and then rely on the user or a separate process to run the experiment script, an agent might also need to trigger experiment execution programmatically. The primary way to do this is by invoking the command-line interface (CLI) of `run_experiment.py` or `train_model.py`.

## Using `subprocess` Module

The standard Python `subprocess` module can be used to launch these scripts.

```python
import subprocess
import os

def run_simplexity_experiment(cli_args: list[str], project_root: str = "."):
    """
    Runs the simplexity experiment script with the given CLI arguments.

    Args:
        cli_args: A list of command-line arguments and overrides 
                  (e.g., ["training.num_steps=100", "predictive_model=gru_rnn"]).
        project_root: The root directory of the Simplexity project.
    """
    command = ["python", "simplexity/run_experiment.py"] + cli_args
    
    try:
        # Using absolute path for script if project_root is not cwd
        script_path = os.path.join(os.path.abspath(project_root), "simplexity/run_experiment.py")
        command = ["python", script_path] + cli_args
        
        print(f"Executing command: {' '.join(command)}")
        print(f"Working directory: {os.path.abspath(project_root)}")

        # It's crucial to set the working directory if the script relies on relative paths
        # for configs or outputs, which Hydra scripts usually do.
        result = subprocess.run(
            command, 
            cwd=os.path.abspath(project_root), 
            capture_output=True, 
            text=True, 
            check=True # Raises CalledProcessError if the command returns a non-zero exit code
        )
        print("Experiment completed successfully.")
        print("Stdout:")
        print(result.stdout)
        if result.stderr:
            print("Stderr:")
            print(result.stderr)
        return result.stdout, result.stderr
    except subprocess.CalledProcessError as e:
        print(f"Error during experiment execution (Return Code: {e.returncode}):")
        print("Stdout:")
        print(e.stdout)
        print("Stderr:")
        print(e.stderr)
        raise # Re-raise the exception so the caller knows something went wrong
    except FileNotFoundError:
        print(f"Error: Could not find python or simplexity/run_experiment.py. Ensure project_root is correct.")
        raise

# Example Usage for an LLM Agent:
if __name__ == "__main__":
    # Assuming the agent has determined these overrides are needed:
    overrides = [
        "training.num_steps=10", 
        "training.log_every=1",
        "logging=print_logger", # Use print_logger for quick stdout feedback
        "training_data_generator.instance.p=0.6" # Example override for a data generator param
    ]
    try:
        # If the agent is running from the project root:
        run_simplexity_experiment(overrides)
        
        # If the agent knows the project is elsewhere:
        # run_simplexity_experiment(overrides, project_root="/path/to/simplexity_project")
    except Exception as e:
        print(f"Agent-level error: {e}")

```

## Key Considerations for Programmatic Execution:

1.  **Working Directory (`cwd`):**
    *   Hydra scripts are very sensitive to the current working directory. They expect to be run from the project root where the `simplexity/configs/` directory and the main script are located.
    *   When using `subprocess.run`, **always specify the `cwd` (current working directory) argument** to be the root of the Simplexity project.

2.  **Command-Line Arguments:**
    *   All Hydra overrides discussed in `run_experiment_script.md` and `interacting_with_configs.md` can be passed as strings in the `cli_args` list.
    *   Examples: `"seed=42"`, `"training.num_steps=10000"`, `"predictive_model=my_other_model"`, `"training.optimizer.instance.learning_rate=0.0001"`.
    *   For multi-run/sweeps: `"--multirun"`, `"train.batch_size=32,64,128"`.

3.  **Error Handling:**
    *   Use `check=True` in `subprocess.run` to automatically raise a `CalledProcessError` if the script exits with a non-zero status code (indicating an error).
    *   Capture and examine `stdout` and `stderr` from the completed process to understand the outcome or diagnose issues.

4.  **Output Parsing:**
    *   Hydra typically creates output directories (e.g., `outputs/YYYY-MM-DD/HH-MM-SS/` or `multirun/...`). An LLM agent might need to know these locations to find logs or artifacts. The exact output path can sometimes be inferred from Hydra's stdout or by knowing its default patterns.
    *   If using `print_logger`, essential information might be directly in `result.stdout`.

5.  **Virtual Environment:**
    *   The `python` executable used by `subprocess.run` should be the one from the correct virtual environment where all dependencies (JAX, Hydra, Equinox, etc.) are installed. If the agent is itself running in that environment, simply using `"python"` is usually fine. Otherwise, the full path to the venv's Python interpreter might be needed.

6.  **Blocking vs. Non-Blocking:**
    *   `subprocess.run()` is blocking; it waits for the experiment to complete.
    *   For long-running experiments, an agent might consider using `subprocess.Popen()` for non-blocking execution, which would require more complex management of the child process.

7.  **Security:**
    *   Be extremely cautious if any part of the command or arguments comes from untrusted user input, as this can lead to command injection vulnerabilities. For LLM agents generating these commands based on internal logic or trusted instructions, this is less of a direct concern but good to be aware of.

By using `subprocess` carefully, an LLM agent can effectively launch and manage experiment runs, bridging the gap between configuration modification and actual execution. 