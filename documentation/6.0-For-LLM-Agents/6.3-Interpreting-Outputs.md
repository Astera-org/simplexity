# 6.3. Interpreting Outputs and Logs

After an experiment run (whether a single run or a trial within a hyperparameter sweep) is executed, an LLM agent needs to be able to locate and interpret its outputs and logs to understand the results, find saved artifacts, or diagnose issues.

## Standard Output Directory Structure (Hydra)

Hydra manages the output directories. By default, for a single run (not part of a sweep), outputs are saved to:
`outputs/<YYYY-MM-DD>/<HH-MM-SS>/`
(Relative to the directory where `run_experiment.py` was executed, usually the project root).

For a multi-run (hyperparameter sweep), the structure is typically:
`multirun/<YYYY-MM-DD>/<HH-MM-SS-ms>/<trial_id>/`
where `<trial_id>` is a number (0, 1, 2...). The exact path can be customized with Hydra settings (`hydra.sweep.dir`, `hydra.run.dir`).

**Inside each individual run/trial directory, an agent can expect to find:**

1.  **`.hydra/` subdirectory:** This is critical for understanding the exact configuration of that run.
    *   `config.yaml`: The fully resolved Hydra configuration for this specific run, after all defaults, overrides, and interpolations.
    *   `overrides.yaml`: A list of the command-line or sweeper-generated overrides that were applied to the base configuration for this run.
    *   `hydra.log`: A log file from Hydra itself, detailing its setup and execution for this run.

2.  **Log Files (from the configured `Logger`):**
    *   **If `FileLogger` was used:** The log file specified in its configuration (e.g., `experiment_log.txt`, or a path relative to this output directory).
        *   **Content:** Will contain plain text records of configs, params, and metrics (step, value pairs) as logged by the `Logger` interface methods.
    *   **If `PrintLogger` was used:** The primary output will be in the `stdout` of the experiment script execution. The agent would need to have captured this (e.g., from `subprocess.run().stdout`).
    *   **If `MLFlowLogger` was used:**
        *   MLflow logs are primarily sent to the MLflow Tracking Server (specified by `tracking_uri`). The agent would need to interact with the MLflow API (using the `mlflow` Python client) or have access to the MLflow UI to retrieve detailed logs and metrics for the run (identified by its MLflow Run ID, which `MLFlowLogger` usually prints to stdout or could be made to return).
        *   Sometimes, `MLFlowLogger` might also save a local copy of some info or a reference, but the main source is the server.
        *   The `.hydra/config.yaml` is still valuable as MLflow typically logs parameters and metrics but might not store the *entire* raw Hydra config in an easily parsable way within its own UI/database for all fields.

3.  **Model Checkpoints (from the configured `ModelPersister`):**
    *   The location and naming convention are defined by the persister's configuration (e.g., `persistence.instance.directory` and `persistence.instance.filename`).
    *   If the default `directory` was relative (e.g., `./model_checkpoints`), these will be inside the current run's output directory (e.g., `outputs/.../.../model_checkpoints/model_step_1000.eqx`).
    *   If the `directory` was an absolute path, they will be there instead.
    *   The agent needs to parse the `persistence` configuration to determine the exact path and filename pattern (which often includes `{step}`).

## Interpreting Information

*   **Experiment Success/Failure:**
    *   Check the return code of the `subprocess.run()` call. Zero usually means success.
    *   Examine `stderr` for any error messages or stack traces.

*   **Key Metrics:**
    *   For `FileLogger` or `PrintLogger` (from `stdout`), parse the metric lines, which typically look like `Step: <step_number>, Metrics: {'loss': <value>, 'validation/loss': <value>, ...}`.
    *   For `MLFlowLogger`, query the MLflow API for metrics associated with the run ID.
    *   The final loss is often returned by `run_experiment.py` and might be printed to `stdout`.

*   **Best Hyperparameters (for sweeps):**
    *   Hydra + Optuna often print a summary of the best trial and its parameters to `stdout` at the end of a sweep.
    *   If MLflow is used, the agent can query MLflow for all trials in the experiment, sort by the target metric (e.g., `validation/loss`), and identify the parameters of the best run from its `params` log.
    *   The configuration for the best trial can be found in its specific `multirun/.../<best_trial_id>/.hydra/config.yaml`.

*   **Identifying Saved Model Files:**
    *   From the `persistence` configuration in `.hydra/config.yaml` (or the main config if not overridden), extract `directory` and `filename` pattern.
    *   The training loop logs when it saves checkpoints (e.g., "Saving checkpoint at step X"). This can help confirm which steps have saved models.
    *   To load a specific model, the agent needs the path to the checkpoint file and the model architecture must match.

## Practical Steps for an LLM Agent

1.  **Determine the Output Directory:** After an experiment command, the first step is to identify the run's output directory. If the command was `python simplexity/run_experiment.py`, and it completed at `2023-10-27 14:30:00`, the directory is likely `outputs/2023-10-27/14-30-00/`.
2.  **Load `.hydra/config.yaml`:** This file is the ground truth for what was run.
3.  **Check `stdout` / `stderr`:** Parse for explicit success/failure messages, final metrics, or error traces.
4.  **Access Logger Output:** Based on `logging.instance._target_` from the config:
    *   If `FileLogger`, read the specified `file_path` (relative to the output directory).
    *   If `MLFlowLogger`, note the MLflow run ID (often printed to stdout) and prepare to query MLflow if detailed metrics or many runs need analysis.
5.  **Locate Checkpoints:** Based on `persistence.instance.directory` and `persistence.instance.filename` from the config, construct paths to expected checkpoint files.

By following these steps, an LLM agent can systematically find and interpret the results and artifacts of experiments run by the Simplexity framework. 