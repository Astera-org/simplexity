# 5.3. Customizing Training Loops

The Simplexity framework provides a standard training loop in `simplexity.training.train_equinox_model.py` that caters to many common sequence modeling tasks with Equinox models. However, you might encounter scenarios requiring a more specialized training procedure.

## When to Customize

Consider customizing the training loop if you need to:

*   **Implement a different optimization algorithm:** Beyond what Optax offers or requiring a more complex interaction with the model (e.g., some types of meta-learning, or algorithms that don't fit the standard `optimizer.update(grads, opt_state, params)` pattern).
*   **Use a non-standard loss function:** If your loss requires more than just `logits` and `labels`, or if it involves complex computations not easily encapsulated in the `loss_fn` of `train_equinox_model.py`.
*   **Introduce custom data loading/batching:** If the `generate_data_batch` function is insufficient for your data source or if you need more complex batching strategies (e.g., curriculum learning based on batch content).
*   **Modify the gradient computation/application:** For example, custom gradient clipping, gradient accumulation over multiple batches before an update, or more intricate distributed training strategies beyond simple data parallelism.
*   **Add model-specific operations within the training step:** Such as internal state updates in the model that aren't part of the forward pass, or regularization techniques applied directly during the step.
*   **Train models not based on Equinox (e.g., pure Penzai models with specific training needs):** The presence of `train_penzai_model.py` suggests the framework anticipates this.

## Approaches to Customization

1.  **Minor Modifications to `train_equinox_model.py` (Fork and Modify):**
    *   **Pros:** Easiest for small changes.
    *   **Cons:** Can lead to divergence if the base `train_equinox_model.py` evolves. Might make your changes harder to maintain or share.
    *   **How:**
        1.  Make a copy of `train_equinox_model.py` (e.g., `my_custom_train_loop.py`).
        2.  Modify its internal functions (`generate_data_batch`, `loss_fn`, `update_model`, `training_step`, or the main `train` function) as needed.
        3.  In your `run_experiment.py` (or a copy of it), change the import and call to point to your new `my_custom_train_loop.train` function.
            ```python
            # In your custom run_experiment.py
            # from simplexity.training.train_equinox_model import train
            from path.to.my_custom_train_loop import train as custom_train

            # ... later in the script
            # _, loss = train(...)
            _, loss = custom_train(...)
            ```
        4.  You might need to adjust the `TrainingConfig` or create a new one if your custom loop requires different configuration parameters.

2.  **Creating a New, Separate Training Script (e.g., `train_my_model_type.py`):**
    *   **Pros:** Clean separation, good for substantially different training regimes or model types (like the existing `train_penzai_model.py`). Easier to manage distinct configuration needs.
    *   **Cons:** More boilerplate if there's still significant overlap with existing loops.
    *   **How:**
        1.  Create a new file, say `simplexity/training/train_my_model_type.py`.
        2.  Define a main `train(...)` function within it. This function should accept the `model`, `training_cfg` (which could be a new custom `MyTrainingConfig` dataclass), data generators, logger, persister, etc., similar to the existing `train` functions.
        3.  Implement your custom data generation, loss computation, model update, and overall training loop logic.
        4.  **Configuration:**
            *   Define a new configuration dataclass for your training loop if its parameters differ significantly (e.g., `simplexity/configs/training/my_custom_training_config.py`).
            *   Create a corresponding YAML file (e.g., `simplexity/configs/train/my_custom_train_setup.yaml`) that specifies any new parameters your loop needs.
            *   Update your main experiment script (`run_experiment.py` or a new one) to instantiate your new training config and call your new `train` function.
            *   Alternatively, if your custom training loop is always tied to a specific model type, the model's configuration itself could specify which training script/function to use (though this adds coupling).

3.  **Making the Existing Loop More Pluggable (Advanced Refactoring):**
    *   **Pros:** Can make the framework more flexible for everyone if done well. Reduces code duplication.
    *   **Cons:** Requires careful design and deeper changes to the existing codebase.
    *   **How:** Identify parts of `train_equinox_model.py` that are common sources of customization (e.g., the loss calculation, the optimizer step) and refactor them to accept functions or strategies passed via configuration.
        *   For example, the `loss_fn` could be made a parameter of the `train` function, configured via Hydra to point to different loss implementations.
        *   This is a more involved contribution to the core framework.

## Key Components to Consider When Customizing

*   **Data Generation:** How batches are created. If `generate_data_batch` is not flexible enough, you'll need to replace its calls.
*   **Loss Calculation:** The `loss_fn` in `train_equinox_model.py` is specific to softmax cross-entropy. If you need something else, this must change.
*   **Optimizer and Gradient Application:** The `update_model` function handles standard Optax optimizer updates. More complex schemes will require rewriting this.
*   **Metrics:** The `evaluation_step` and how metrics are calculated might also need adjustment if your model or task has custom performance indicators.
*   **Configuration (`TrainingConfig`):** Ensure your custom loop can be configured cleanly via Hydra, potentially by defining a new `TrainingConfig` dataclass if new parameters are needed.

## Example: `train_penzai_model.py`

The existence of `simplexity/training/train_penzai_model.py` already demonstrates the pattern of having separate training scripts for potentially different model types or training paradigms. Examining its structure and how it differs from `train_equinox_model.py` can provide a good example of approach #2.

When customizing, always aim for clarity and maintainability. If your changes are broadly useful, consider how they might be contributed back to the core framework. 