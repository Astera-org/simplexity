name: hooked_transformer
instance:
  _target_: transformer_lens.HookedTransformer
  cfg:
    _target_: transformer_lens.HookedTransformerConfig
    d_model: 512
    d_head: 128
    n_heads: 4
    n_layers: 2
    n_ctx: 10
    d_mlp: 2048
    d_vocab: ???
    act_fn: "relu"
    normalization_type: "LN"
    device: ${device}
    seed: ${seed}

load_checkpoint_step: null
