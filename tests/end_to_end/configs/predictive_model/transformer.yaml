name: hooked_transformer
instance:
  _target_: transformer_lens.HookedTransformer
  cfg:
    _target_: transformer_lens.HookedTransformerConfig
    d_model: 256
    d_head: 64
    n_heads: 4
    n_layers: 2
    n_ctx: 64
    d_mlp: 1024
    d_vocab: ???
    act_fn: "relu"
    normalization_type: "LN"
    device: ${device}
    seed: ${seed}

load_checkpoint_step: null
