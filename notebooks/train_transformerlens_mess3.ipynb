{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TransformerLens on the mess3 Process\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Use the simplexity library to generate data from the mess3 Hidden Markov Model\n",
    "2. Train a TransformerLens model on this data\n",
    "3. Use TransformerLens's interpretability features to analyze what the model learned\n",
    "\n",
    "The mess3 process is a 3-state Hidden Markov Model that generates sequences with specific transition patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Quick Start for Google Colab\n\n**Option 1: Open this notebook directly in Colab**\n- Go to: https://colab.research.google.com/github/Astera-org/simplexity/blob/MATS_2025_app/notebooks/train_transformerlens_mess3.ipynb\n\n**Option 2: Download and run in any Colab notebook**\n```python\n# Download this notebook to Colab\n!wget https://raw.githubusercontent.com/Astera-org/simplexity/MATS_2025_app/notebooks/train_transformerlens_mess3.ipynb\n```\n\nThen File → Open → Upload the downloaded notebook",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simple installation for Google Colab\n# Install from the MATS_2025_app branch with TransformerLens support\n\n!pip install git+https://github.com/adamimos/simplexity.git@MATS_2025_app#egg=simplexity[transformerlens] -q\n\nprint(\"✅ Installation complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Do NOT import anything before this cell.\n%pip -q install --upgrade pip wheel setuptools\n\n# Keep Colab's NumPy 2.x to avoid ABI hell.\n# Install minimal, pure-Python deps that TransformerLens actually uses.\n%pip -q install \"einops>=0.7.0\" \"jaxtyping>=0.2.28\" \"beartype>=0.14\"\n\n# Install TransformerLens WITHOUT pulling its optional heavy deps (e.g. HF transformers).\n# Pin to a 2025 build that supports Python 3.12+ and works fine with NumPy 2.x.\n%pip -q install --no-deps \"transformer-lens>=2.16.1\"\n\n# Install simplexity from your branch (without extras so it doesn't try to re-resolve TL)\n%pip -q install \"git+https://github.com/Astera-org/simplexity.git@MATS_2025_app\"\n\n# Install better_abc (required by TransformerLens)\n%pip -q install better_abc\n\nprint(\"✅ Installation complete! You can now run the import cell.\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Alternative: One-line installation (run after restart)\n!pip install numpy==1.24.3 git+https://github.com/Astera-org/simplexity.git@MATS_2025_app#egg=simplexity[transformerlens] -q\n\nprint(\"✅ Ready to go!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Optional\nimport jax\nimport jax.numpy as jnp\n\n# TransformerLens imports\nfrom transformer_lens import HookedTransformer, HookedTransformerConfig\nfrom transformer_lens import utils as tl_utils\n\n# Simplexity imports\nfrom simplexity.generative_processes.builder import build_hidden_markov_model\nfrom simplexity.generative_processes.torch_generator import generate_data_batch\nfrom simplexity.predictive_models.transformerlens_model import TransformerLensWrapper\n\n# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif device.type == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mess3 process\n",
    "mess3 = build_hidden_markov_model(\"mess3\", x=0.15, a=0.6)\n",
    "\n",
    "print(f\"mess3 process created:\")\n",
    "print(f\"  Vocabulary size: {mess3.vocab_size}\")\n",
    "print(f\"  Number of states: {mess3.num_states}\")\n",
    "print(f\"  Initial state shape: {mess3.initial_state.shape}\")\n",
    "\n",
    "# Get the stationary distribution\n",
    "stationary_state = mess3.stationary_state\n",
    "print(f\"\\nStationary distribution: {stationary_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure the TransformerLens Model\n",
    "\n",
    "We'll create a small transformer suitable for learning the mess3 patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_config = {\n",
    "    \"d_model\": 64,           # Model dimension\n",
    "    \"d_head\": 16,            # Head dimension  \n",
    "    \"n_heads\": 4,            # Number of attention heads\n",
    "    \"n_layers\": 2,           # Number of transformer layers\n",
    "    \"n_ctx\": 64,             # Context window\n",
    "    \"d_vocab\": mess3.vocab_size,  # Vocabulary size (3 for mess3)\n",
    "    \"act_fn\": \"relu\",        # Activation function\n",
    "    \"normalization_type\": \"LN\",  # Layer normalization\n",
    "    \"device\": str(device),\n",
    "    \"seed\": 42,\n",
    "    \"attn_only\": False,      # Include MLPs\n",
    "    \"use_cache\": True,       # Enable caching for interpretability\n",
    "    \"use_hook_tokens\": True, # Enable hook points for analysis\n",
    "}\n",
    "\n",
    "# Create the model using the wrapper\n",
    "model = TransformerLensWrapper(**model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created with {total_params:,} parameters\")\n",
    "print(f\"Model configuration:\")\n",
    "for key, value in model_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Generation Function\n",
    "\n",
    "Create a function to generate training batches from the mess3 process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_batch(\n",
    "    generator,\n",
    "    batch_size: int,\n",
    "    sequence_len: int,\n",
    "    key: jax.random.PRNGKey,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generate a batch of training data from the mess3 process.\n",
    "    \n",
    "    Args:\n",
    "        generator: The generative process (mess3)\n",
    "        batch_size: Number of sequences in the batch\n",
    "        sequence_len: Length of each sequence\n",
    "        key: JAX random key\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (inputs, labels) as PyTorch tensors\n",
    "    \"\"\"\n",
    "    # Initialize generator states\n",
    "    gen_state = generator.initial_state\n",
    "    gen_states = jnp.repeat(gen_state[None, :], batch_size, axis=0)\n",
    "    \n",
    "    # Generate data using simplexity's torch_generator\n",
    "    gen_states, inputs, labels = generate_data_batch(\n",
    "        gen_states,\n",
    "        generator,\n",
    "        batch_size,\n",
    "        sequence_len,\n",
    "        key,\n",
    "        bos_token=None,\n",
    "        eos_token=None,\n",
    "    )\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    inputs_torch = torch.from_numpy(np.array(inputs)).long().to(device)\n",
    "    labels_torch = torch.from_numpy(np.array(labels)).long().to(device)\n",
    "    \n",
    "    return inputs_torch, labels_torch\n",
    "\n",
    "# Test data generation\n",
    "test_key = jax.random.PRNGKey(0)\n",
    "test_inputs, test_labels = generate_training_batch(mess3, batch_size=4, sequence_len=10, key=test_key)\n",
    "print(f\"Generated batch shape: {test_inputs.shape}\")\n",
    "print(f\"Sample sequence: {test_inputs[0].cpu().numpy()}\")\n",
    "print(f\"Sample labels: {test_labels[0].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Train the TransformerLens model on data from the mess3 process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformerlens_on_mess3(\n",
    "    model: TransformerLensWrapper,\n",
    "    generator,\n",
    "    num_steps: int = 1000,\n",
    "    batch_size: int = 32,\n",
    "    sequence_len: int = 64,\n",
    "    learning_rate: float = 1e-3,\n",
    "    log_every: int = 100,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Train TransformerLens model on mess3 data.\n",
    "    \n",
    "    Args:\n",
    "        model: TransformerLens model wrapper\n",
    "        generator: mess3 generative process\n",
    "        num_steps: Number of training steps\n",
    "        batch_size: Batch size\n",
    "        sequence_len: Sequence length\n",
    "        learning_rate: Learning rate for Adam optimizer\n",
    "        log_every: Log metrics every N steps\n",
    "        seed: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        List of losses\n",
    "    \"\"\"\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize JAX random key\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    # Training metrics\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"Starting training for {num_steps} steps...\")\n",
    "    print(f\"Batch size: {batch_size}, Sequence length: {sequence_len}\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Generate new batch\n",
    "        key, batch_key = jax.random.split(key)\n",
    "        inputs, labels = generate_training_batch(\n",
    "            generator, batch_size, sequence_len, batch_key\n",
    "        )\n",
    "        \n",
    "        # Forward pass - TransformerLens computes loss internally\n",
    "        # For next-token prediction, we use the inputs as both input and target\n",
    "        loss = model.model(inputs, return_type=\"loss\")\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        loss_value = loss.item()\n",
    "        losses.append(loss_value)\n",
    "        \n",
    "        # Logging\n",
    "        if (step + 1) % log_every == 0:\n",
    "            avg_loss = np.mean(losses[-log_every:])\n",
    "            print(f\"Step {step + 1}/{num_steps}: Loss = {loss_value:.4f}, Avg = {avg_loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTraining completed! Final loss: {losses[-1]:.4f}\")\n",
    "    return losses\n",
    "\n",
    "# Train the model\n",
    "training_losses = train_transformerlens_on_mess3(\n",
    "    model=model,\n",
    "    generator=mess3,\n",
    "    num_steps=1000,\n",
    "    batch_size=32,\n",
    "    sequence_len=64,\n",
    "    learning_rate=1e-3,\n",
    "    log_every=100,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_losses, alpha=0.7)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('TransformerLens Training on mess3 Process')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add smoothed curve\n",
    "window_size = 50\n",
    "if len(training_losses) > window_size:\n",
    "    smoothed = np.convolve(training_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(window_size//2, len(training_losses)-window_size//2+1), \n",
    "             smoothed, 'r-', linewidth=2, label='Smoothed')\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {training_losses[0]:.4f}\")\n",
    "print(f\"Final loss: {training_losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {(training_losses[0] - training_losses[-1]) / training_losses[0] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Evaluate the trained model on fresh data from the mess3 process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, generator, num_batches=10, batch_size=32, sequence_len=64, seed=999):\n",
    "    \"\"\"Evaluate model on fresh data.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_batches):\n",
    "            key, batch_key = jax.random.split(key)\n",
    "            inputs, labels = generate_training_batch(\n",
    "                generator, batch_size, sequence_len, batch_key\n",
    "            )\n",
    "            \n",
    "            # Get model predictions\n",
    "            logits = model(inputs, return_loss=False)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits[:, :-1].reshape(-1, logits.size(-1)),\n",
    "                inputs[:, 1:].reshape(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = logits[:, :-1].argmax(dim=-1)\n",
    "            correct = (predictions == inputs[:, 1:]).sum().item()\n",
    "            total_correct += correct\n",
    "            total_tokens += inputs[:, 1:].numel()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = total_correct / total_tokens\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = evaluate_model(model, mess3)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2%}\")\n",
    "\n",
    "# Compare with random baseline\n",
    "random_accuracy = 1.0 / mess3.vocab_size\n",
    "print(f\"\\nRandom baseline accuracy: {random_accuracy:.2%}\")\n",
    "print(f\"Improvement over random: {(val_accuracy - random_accuracy) / random_accuracy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interpretability Analysis with TransformerLens\n",
    "\n",
    "Use TransformerLens's built-in features to analyze what the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample sequence for analysis\n",
    "model.eval()\n",
    "analysis_key = jax.random.PRNGKey(123)\n",
    "sample_inputs, _ = generate_training_batch(mess3, batch_size=1, sequence_len=20, key=analysis_key)\n",
    "\n",
    "print(f\"Analyzing sequence: {sample_inputs[0].cpu().numpy()}\")\n",
    "\n",
    "# Run with cache to get all activations\n",
    "logits, cache = model.run_with_cache(sample_inputs)\n",
    "\n",
    "# Get predictions\n",
    "predictions = logits[0].argmax(dim=-1).cpu().numpy()\n",
    "print(f\"Model predictions: {predictions[:-1]}\")\n",
    "print(f\"Actual next tokens: {sample_inputs[0, 1:].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention Pattern Visualization\n",
    "\n",
    "Visualize the attention patterns learned by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention patterns for each layer\n",
    "n_layers = model.config.n_layers\n",
    "n_heads = model.config.n_heads\n",
    "\n",
    "fig, axes = plt.subplots(n_layers, n_heads, figsize=(n_heads * 4, n_layers * 4))\n",
    "if n_layers == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "if n_heads == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    # Get attention patterns for this layer\n",
    "    attn_patterns = cache[\"pattern\", layer][0].cpu().numpy()  # Shape: (n_heads, seq_len, seq_len)\n",
    "    \n",
    "    for head in range(n_heads):\n",
    "        ax = axes[layer, head] if n_layers > 1 and n_heads > 1 else axes[max(layer, head)]\n",
    "        \n",
    "        # Plot attention pattern\n",
    "        im = ax.imshow(attn_patterns[head], cmap='Blues', aspect='auto')\n",
    "        ax.set_title(f'Layer {layer}, Head {head}')\n",
    "        ax.set_xlabel('Source Position')\n",
    "        ax.set_ylabel('Target Position')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "plt.suptitle('Attention Patterns Across Layers and Heads', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze attention focus\n",
    "for layer in range(n_layers):\n",
    "    attn_patterns = cache[\"pattern\", layer][0].cpu().numpy()\n",
    "    avg_attention = attn_patterns.mean(axis=0)  # Average across heads\n",
    "    \n",
    "    # Calculate entropy to measure attention focus\n",
    "    entropy = -np.sum(avg_attention * np.log(avg_attention + 1e-10), axis=-1)\n",
    "    print(f\"Layer {layer} - Average attention entropy: {entropy.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Model's Internal Representations\n",
    "\n",
    "Examine how the model represents the mess3 states internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings and residual stream activations\n",
    "embed = cache[\"embed\"][0].cpu().numpy()  # Token embeddings\n",
    "final_residual = cache[\"resid_post\", n_layers - 1][0].cpu().numpy()  # Final layer residual\n",
    "\n",
    "# Analyze token embeddings\n",
    "print(\"Token Embedding Analysis:\")\n",
    "print(f\"Embedding shape: {embed.shape}\")\n",
    "print(f\"Final residual shape: {final_residual.shape}\")\n",
    "\n",
    "# Calculate cosine similarity between token embeddings\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Get unique token embeddings\n",
    "unique_tokens = list(range(mess3.vocab_size))\n",
    "token_embeds = model.model.embed.W_E.detach().cpu().numpy()\n",
    "\n",
    "print(\"\\nCosine similarity between token embeddings:\")\n",
    "sim_matrix = np.zeros((len(unique_tokens), len(unique_tokens)))\n",
    "for i, token_i in enumerate(unique_tokens):\n",
    "    for j, token_j in enumerate(unique_tokens):\n",
    "        sim = cosine_similarity(token_embeds[token_i], token_embeds[token_j])\n",
    "        sim_matrix[i, j] = sim\n",
    "        if i < j:\n",
    "            print(f\"  Token {token_i} vs Token {token_j}: {sim:.3f}\")\n",
    "\n",
    "# Visualize similarity matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(sim_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Token')\n",
    "plt.title('Token Embedding Similarity Matrix')\n",
    "plt.xticks(range(len(unique_tokens)), unique_tokens)\n",
    "plt.yticks(range(len(unique_tokens)), unique_tokens)\n",
    "for i in range(len(unique_tokens)):\n",
    "    for j in range(len(unique_tokens)):\n",
    "        plt.text(j, i, f'{sim_matrix[i, j]:.2f}', ha='center', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Probe Model's Understanding of mess3 Dynamics\n",
    "\n",
    "Test if the model learned the underlying transition structure of mess3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transition_predictions(model, generator, num_samples=1000, seed=456):\n",
    "    \"\"\"Analyze how well the model predicts transitions.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Track predictions for each token pair\n",
    "    transition_counts = np.zeros((generator.vocab_size, generator.vocab_size))\n",
    "    transition_correct = np.zeros((generator.vocab_size, generator.vocab_size))\n",
    "    \n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples // 32):\n",
    "            key, batch_key = jax.random.split(key)\n",
    "            inputs, _ = generate_training_batch(generator, 32, 64, batch_key)\n",
    "            \n",
    "            logits = model(inputs, return_loss=False)\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            \n",
    "            # Count transitions\n",
    "            for i in range(inputs.shape[1] - 1):\n",
    "                current_tokens = inputs[:, i].cpu().numpy()\n",
    "                next_tokens = inputs[:, i + 1].cpu().numpy()\n",
    "                predicted_tokens = predictions[:, i].cpu().numpy()\n",
    "                \n",
    "                for curr, next_tok, pred in zip(current_tokens, next_tokens, predicted_tokens):\n",
    "                    transition_counts[curr, next_tok] += 1\n",
    "                    if pred == next_tok:\n",
    "                        transition_correct[curr, next_tok] += 1\n",
    "    \n",
    "    # Calculate accuracy for each transition\n",
    "    transition_accuracy = np.divide(\n",
    "        transition_correct,\n",
    "        transition_counts,\n",
    "        out=np.zeros_like(transition_correct),\n",
    "        where=transition_counts > 0\n",
    "    )\n",
    "    \n",
    "    # Normalize to get empirical transition probabilities\n",
    "    empirical_transitions = transition_counts / transition_counts.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return transition_accuracy, empirical_transitions, transition_counts\n",
    "\n",
    "# Analyze transitions\n",
    "trans_acc, emp_trans, trans_counts = analyze_transition_predictions(model, mess3)\n",
    "\n",
    "# Visualize transition accuracy\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot empirical transitions\n",
    "im1 = axes[0].imshow(emp_trans, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_title('Empirical Transition Probabilities\\n(from generated data)')\n",
    "axes[0].set_xlabel('Next Token')\n",
    "axes[0].set_ylabel('Current Token')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "for i in range(mess3.vocab_size):\n",
    "    for j in range(mess3.vocab_size):\n",
    "        axes[0].text(j, i, f'{emp_trans[i, j]:.2f}', ha='center', va='center')\n",
    "\n",
    "# Plot prediction accuracy\n",
    "im2 = axes[1].imshow(trans_acc, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[1].set_title('Model Prediction Accuracy\\nfor Each Transition')\n",
    "axes[1].set_xlabel('Next Token')\n",
    "axes[1].set_ylabel('Current Token')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "for i in range(mess3.vocab_size):\n",
    "    for j in range(mess3.vocab_size):\n",
    "        if trans_counts[i, j] > 0:\n",
    "            axes[1].text(j, i, f'{trans_acc[i, j]:.2f}', ha='center', va='center')\n",
    "\n",
    "# Plot count distribution\n",
    "im3 = axes[2].imshow(trans_counts, cmap='YlOrRd')\n",
    "axes[2].set_title('Transition Counts\\n(total observations)')\n",
    "axes[2].set_xlabel('Next Token')\n",
    "axes[2].set_ylabel('Current Token')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "for i in range(mess3.vocab_size):\n",
    "    for j in range(mess3.vocab_size):\n",
    "        axes[2].text(j, i, f'{int(trans_counts[i, j])}', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "overall_accuracy = trans_correct.sum() / trans_counts.sum()\n",
    "print(f\"\\nOverall transition prediction accuracy: {overall_accuracy:.2%}\")\n",
    "print(f\"\\nPer-token accuracy:\")\n",
    "for i in range(mess3.vocab_size):\n",
    "    token_acc = trans_correct[i].sum() / trans_counts[i].sum() if trans_counts[i].sum() > 0 else 0\n",
    "    print(f\"  Token {i}: {token_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. How to use simplexity to generate data from the mess3 Hidden Markov Model\n",
    "2. How to train a TransformerLens model on this synthetic data\n",
    "3. How to use TransformerLens's interpretability features to understand what the model learned\n",
    "\n",
    "Key observations:\n",
    "- The model successfully learns to predict the next token in mess3 sequences\n",
    "- Attention patterns reveal how the model tracks dependencies in the sequence\n",
    "- The model's internal representations capture the structure of the underlying HMM\n",
    "\n",
    "This approach can be extended to:\n",
    "- Other generative processes in simplexity\n",
    "- Larger transformer models\n",
    "- More complex analysis using TransformerLens's advanced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model if needed\n",
    "save_model = False  # Set to True to save\n",
    "\n",
    "if save_model:\n",
    "    model_path = \"transformerlens_mess3_model.pt\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.model.state_dict(),\n",
    "        'config': model_config,\n",
    "        'training_losses': training_losses,\n",
    "    }, model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}