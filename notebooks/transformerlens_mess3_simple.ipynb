{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerLens + mess3: Simple Training Example\n",
    "\n",
    "Train a small transformer on the mess3 Hidden Markov Model using TransformerLens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Installation (skip if already installed)\n",
    "%pip -q install --upgrade pip wheel setuptools\n",
    "%pip -q install \"einops>=0.7.0\" \"jaxtyping>=0.2.28\" \"beartype>=0.14\" better_abc\n",
    "%pip -q install --no-deps \"transformer-lens>=2.16.1\"\n",
    "%pip -q install \"git+https://github.com/Astera-org/simplexity.git@MATS_2025_app\"\n",
    "print(\"✅ Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Setup - Product of tom_quantum and mess3\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom transformer_lens import HookedTransformer, HookedTransformerConfig\nfrom simplexity.generative_processes.builder import build_hidden_markov_model\nfrom simplexity.generative_processes.torch_generator import generate_data_batch\nfrom simplexity.generative_processes.hidden_markov_model import HiddenMarkovModel\nfrom simplexity.generative_processes.transition_matrices import tom_quantum as tom_quantum_matrix\n\n# Create mess3 process\nmess3 = build_hidden_markov_model(\"mess3\", x=0.15, a=0.6)\n\n# Create tom_quantum manually since it's not in the builder\ntom_transition = tom_quantum_matrix(alpha=1, beta=51)\ntom_quantum = HiddenMarkovModel(transition_matrices=tom_transition)\n\nprint(f\"mess3: vocab_size={mess3.vocab_size}, states={mess3.num_states}\")\nprint(f\"tom_quantum: vocab_size={tom_quantum.vocab_size}, states={tom_quantum.num_states}\")\n\n# Product space has vocab_size = 2 * 3 = 6\nproduct_vocab_size = tom_quantum.vocab_size * mess3.vocab_size\nprint(f\"Product space: vocab_size={product_vocab_size}\")\n\n# Create TransformerLens model for product space\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ncfg = HookedTransformerConfig(\n    d_model=128,  # Bigger model for more complex data\n    n_heads=4,\n    n_layers=2,\n    n_ctx=32,\n    d_vocab=product_vocab_size,  # 6 tokens\n    act_fn=\"relu\",\n    device=device,\n)\nmodel = HookedTransformer(cfg)\nprint(f\"Model: {sum(p.numel() for p in model.parameters()):,} params on {device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Train on Product Space\nfrom tqdm import tqdm\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nlosses = []\nbatch_size, seq_len = 32, 32\nkey = jax.random.PRNGKey(42)\n\n# Get stationary distributions\ntom_stationary = tom_quantum.stationary_state\nmess3_stationary = mess3.stationary_state\n\nfor step in tqdm(range(1000), desc=\"Training\"):  # More steps for harder task\n    # Generate NEW batch each time\n    key, key1, key2 = jax.random.split(key, 3)\n    \n    # Generate from tom_quantum\n    tom_states = jnp.repeat(tom_stationary[None, :], batch_size, axis=0)\n    _, tom_inputs, _ = generate_data_batch(tom_states, tom_quantum, batch_size, seq_len, key1)\n    \n    # Generate from mess3\n    mess3_states = jnp.repeat(mess3_stationary[None, :], batch_size, axis=0)\n    _, mess3_inputs, _ = generate_data_batch(mess3_states, mess3, batch_size, seq_len, key2)\n    \n    # Combine into product space: token = tom * 3 + mess3\n    # This maps (tom=0,mess3=0)->0, (0,1)->1, (0,2)->2, (1,0)->3, (1,1)->4, (1,2)->5\n    if isinstance(tom_inputs, torch.Tensor):\n        tom_arr = tom_inputs.cpu().numpy()\n        mess3_arr = mess3_inputs.cpu().numpy()\n    else:\n        tom_arr = np.array(tom_inputs)\n        mess3_arr = np.array(mess3_inputs)\n    \n    product_tokens = tom_arr * 3 + mess3_arr\n    tokens = torch.from_numpy(product_tokens).long().to(device)\n    \n    # Train step\n    loss = model(tokens, return_type=\"loss\")\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n\nprint(f\"\\nFinal loss: {losses[-1]:.4f} (started at {losses[0]:.4f})\")\n\n# Visualization\nimport matplotlib.pyplot as plt\nplt.plot(losses)\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.title('Training Loss on Product Space (tom_quantum × mess3)')\nplt.show()"
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Extract activations from residual stream (Product Space)\nfrom sklearn.decomposition import PCA\nimport plotly.graph_objects as go\n\n# Generate a batch for analysis\nkey, key1, key2 = jax.random.split(key, 3)\n\n# Generate product space data\ntom_states = jnp.repeat(tom_stationary[None, :], 100, axis=0)\n_, tom_inputs, _ = generate_data_batch(tom_states, tom_quantum, 100, seq_len, key1)\n\nmess3_states = jnp.repeat(mess3_stationary[None, :], 100, axis=0)\n_, mess3_inputs, _ = generate_data_batch(mess3_states, mess3, 100, seq_len, key2)\n\n# Convert and combine\nif isinstance(tom_inputs, torch.Tensor):\n    tom_arr = tom_inputs.cpu().numpy()\n    mess3_arr = mess3_inputs.cpu().numpy()\nelse:\n    tom_arr = np.array(tom_inputs)\n    mess3_arr = np.array(mess3_inputs)\n\nproduct_tokens_np = tom_arr * 3 + mess3_arr\ntokens = torch.from_numpy(product_tokens_np).long().to(device)\n\n# Run with cache to get all activations\nlogits, cache = model.run_with_cache(tokens)\n\n# Extract residual stream activations\nresidual_streams = {\n    'embeddings': cache['hook_embed'],\n    'layer_0': cache['blocks.0.hook_resid_post'],\n    'layer_1': cache['blocks.1.hook_resid_post'],\n}\n\nprint(\"Activation shapes:\")\nfor name, acts in residual_streams.items():\n    print(f\"  {name}: {acts.shape}\")\n\n# Flatten for PCA\nactivations_flat = {}\nfor name, acts in residual_streams.items():\n    acts_reshaped = acts.reshape(-1, acts.shape[-1]).cpu().numpy()\n    activations_flat[name] = acts_reshaped\n\n# Create labels for visualization\ntoken_labels = tokens.flatten().cpu().numpy()\ntom_labels = token_labels // 3  # Extract tom component\nmess3_labels = token_labels % 3   # Extract mess3 component\n\nprint(f\"\\nTotal points for PCA: {activations_flat['layer_1'].shape[0]}\")\nprint(f\"Token distribution: {np.bincount(token_labels)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: 2D PCA visualization of Product Space\n%pip -q install plotly scikit-learn\n\n# Perform PCA on the final layer activations\npca = PCA(n_components=2)\npca_coords = pca.fit_transform(activations_flat['layer_1'])\n\nprint(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\nprint(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n\n# Create interactive 2D plot with plotly\nfig = go.Figure()\n\n# Color by full token (6 colors)\nimport matplotlib.cm as cm\ncolors = cm.tab10(np.linspace(0, 0.6, 6))\n\nfor token_id in range(6):\n    mask = token_labels == token_id\n    tom_val = token_id // 3\n    mess3_val = token_id % 3\n    \n    fig.add_trace(go.Scatter(\n        x=pca_coords[mask, 0],\n        y=pca_coords[mask, 1],\n        mode='markers',\n        name=f'({tom_val}, {mess3_val})',\n        marker=dict(\n            size=5,\n            color=f'rgb({int(colors[token_id][0]*255)},{int(colors[token_id][1]*255)},{int(colors[token_id][2]*255)})',\n            opacity=0.6,\n        ),\n        text=[f'tom={tom_val}, mess3={mess3_val}' for _ in range(mask.sum())],\n        hovertemplate='%{text}<br>PC1: %{x:.2f}<br>PC2: %{y:.2f}'\n    ))\n\nfig.update_layout(\n    title='2D PCA of Product Space (tom_quantum × mess3)',\n    xaxis_title='PC1',\n    yaxis_title='PC2',\n    height=600,\n    width=800,\n    showlegend=True\n)\n\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Compare PCA across layers (2D)\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\ncolors = ['red', 'green', 'blue']\n\nfor idx, layer_name in enumerate(['embeddings', 'layer_0', 'layer_1']):\n    # PCA for this layer\n    pca = PCA(n_components=2)\n    pca_coords = pca.fit_transform(activations_flat[layer_name])\n    \n    # Plot each token type\n    for token_id in range(3):\n        mask = token_labels == token_id\n        axes[idx].scatter(\n            pca_coords[mask, 0],\n            pca_coords[mask, 1],\n            c=colors[token_id],\n            label=f'Token {token_id}',\n            alpha=0.5,\n            s=10\n        )\n    \n    axes[idx].set_title(f'{layer_name}\\n({sum(pca.explained_variance_ratio_):.1%} var)')\n    axes[idx].set_xlabel('PC1')\n    axes[idx].set_ylabel('PC2')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.suptitle('Token Representations Across Layers (2D PCA)', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Show variance explained progression\nprint(\"Variance explained by first 2 PCs at each layer:\")\nfor layer_name in ['embeddings', 'layer_0', 'layer_1']:\n    pca = PCA(n_components=2)\n    pca.fit(activations_flat[layer_name])\n    print(f\"  {layer_name}: {sum(pca.explained_variance_ratio_):.2%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}