---
description: Model architecture patterns, PredictiveModel protocol, and external library integration
globs: 
alwaysApply: false
---
# Predictive Models

Predictive models in Simplexity take sequences of observations and output probability distributions over next observations.

## Core Protocol

The minimal interface is simple:

```python
@runtime_checkable
class PredictiveModel(Protocol):
    def __call__(self, x: jax.Array, /) -> jax.Array:
        """
        Args:
            x: Input sequence of shape (batch_size, sequence_length)
        Returns:
            Logits of shape (batch_size, sequence_length, vocab_size)
        """
        ...
```

This protocol-based design allows maximum flexibility in implementation.

## Integration Approaches

### Approach A: Direct Library Instantiation (Recommended)

For external libraries (Penzai, Flax, Haiku), use direct instantiation:

```yaml
# configs/predictive_model/penzai_transformer.yaml
name: penzai_transformer
instance:
  _target_: penzai.models.transformer.Transformer
  num_layers: 6
  hidden_size: 256
  num_heads: 8
  vocab_size: ${training_data_generator.instance.vocab_size}
```

**Advantages:**
- No wrapper code needed
- Direct access to all library features
- Easier debugging
- Better performance

### Approach B: Custom Wrapper Functions

Only use when you need preprocessing or custom logic:

```python
# In simplexity/predictive_models/custom_transformer.py
def create_transformer_with_preprocessing(vocab_size: int, **kwargs):
    base_model = TransformerModel(vocab_size=vocab_size, **kwargs)
    
    def model_with_preprocessing(x):
        # Custom preprocessing
        x = preprocess(x)
        # Call base model
        logits = base_model(x)
        # Custom postprocessing
        return postprocess(logits)
    
    return model_with_preprocessing
```

Configuration:
```yaml
instance:
  _target_: simplexity.predictive_models.custom_transformer.create_transformer_with_preprocessing
  vocab_size: ${training_data_generator.instance.vocab_size}
  hidden_size: 256
```

## Built-in Models

### GRU RNN

Basic recurrent model:

```yaml
# configs/predictive_model/gru_rnn.yaml
name: gru_rnn
instance:
  _target_: simplexity.predictive_models.gru_rnn.create_gru_rnn
  vocab_size: ${training_data_generator.instance.vocab_size}
  hidden_size: 128
  num_layers: 2
```

Implementation pattern:
```python
def create_gru_rnn(vocab_size: int, hidden_size: int, num_layers: int):
    return eqx.nn.Sequential([
        eqx.nn.Embedding(vocab_size, hidden_size),
        eqx.nn.GRU(hidden_size, hidden_size, num_layers),
        eqx.nn.Linear(hidden_size, vocab_size),
    ])
```

## Adding New Models

### Step 1: Choose Integration Approach

**Use Approach A if:**
- Using external library (Penzai, Flax, Haiku)
- Model follows standard conventions
- No special preprocessing needed

**Use Approach B if:**
- Need custom preprocessing/postprocessing
- Implementing from scratch
- Combining multiple models

### Step 2: Create Configuration

For Approach A:
```yaml
# configs/predictive_model/external_model.yaml
name: external_model
instance:
  _target_: external_library.models.ModelClass
  # Model parameters
  param1: value1
  param2: value2
  vocab_size: ${training_data_generator.instance.vocab_size}
```

For Approach B:
```yaml
# configs/predictive_model/custom_model.yaml
name: custom_model
instance:
  _target_: simplexity.predictive_models.custom.create_model
  vocab_size: ${training_data_generator.instance.vocab_size}
  # Additional parameters
```

### Step 3: Implement (Approach B only)

```python
# simplexity/predictive_models/custom.py
import equinox as eqx
import jax
import jax.numpy as jnp

def create_model(vocab_size: int, hidden_size: int = 128):
    class CustomModel(eqx.Module):
        embed: eqx.nn.Embedding
        process: eqx.Module
        output: eqx.nn.Linear
        
        def __init__(self, vocab_size, hidden_size):
            self.embed = eqx.nn.Embedding(vocab_size, hidden_size)
            self.process = self._build_processor(hidden_size)
            self.output = eqx.nn.Linear(hidden_size, vocab_size)
        
        def __call__(self, x):
            # x shape: (batch_size, sequence_length)
            embedded = jax.vmap(self.embed)(x)
            processed = self.process(embedded)
            logits = jax.vmap(self.output)(processed)
            return logits
        
        def _build_processor(self, hidden_size):
            # Build processing layers
            return eqx.nn.Sequential([...])
    
    return CustomModel(vocab_size, hidden_size)
```

### Step 4: Register Configuration

Update `configs/predictive_model/config.py`:

```python
@dataclass
class CustomModelConfig:
    _target_: str = "simplexity.predictive_models.custom.create_model"
    vocab_size: int = 2
    hidden_size: int = 128

cs.store(
    group="predictive_model",
    name="custom_model",
    node=PredictiveModelConfig(
        name="custom_model",
        instance=CustomModelConfig(),
    ),
)
```

## Model Patterns

### 1. Embedding-Based Models

```python
def create_embedding_model(vocab_size, embed_size, hidden_size):
    return eqx.nn.Sequential([
        eqx.nn.Embedding(vocab_size, embed_size),
        ProcessingLayer(embed_size, hidden_size),
        eqx.nn.Linear(hidden_size, vocab_size),
    ])
```

### 2. Recurrent Models

```python
def create_rnn_model(vocab_size, hidden_size, cell_type="gru"):
    cells = {
        "gru": eqx.nn.GRU,
        "lstm": eqx.nn.LSTM,
    }
    
    return eqx.nn.Sequential([
        eqx.nn.Embedding(vocab_size, hidden_size),
        cells[cell_type](mdc:hidden_size, hidden_size),
        eqx.nn.Linear(hidden_size, vocab_size),
    ])
```

### 3. Attention-Based Models

Using Penzai:
```yaml
instance:
  _target_: penzai.models.transformer.Transformer
  vocab_size: ${training_data_generator.instance.vocab_size}
  d_model: 256
  num_heads: 8
  num_layers: 6
  d_ff: 1024
  dropout_rate: 0.1
```

### 4. Hybrid Models

Combining architectures:
```python
def create_hybrid_model(vocab_size, hidden_size):
    class HybridModel(eqx.Module):
        rnn: eqx.Module
        attention: eqx.Module
        
        def __call__(self, x):
            rnn_out = self.rnn(x)
            attn_out = self.attention(x)
            combined = jnp.concatenate([rnn_out, attn_out], axis=-1)
            return self.output(combined)
```

## Working with Different Frameworks

### Equinox (Default)

Native support, no conversion needed:
```python
model = eqx.nn.Sequential([...])
# Use directly
```

### Penzai

Direct instantiation via config:
```yaml
instance:
  _target_: penzai.nn.Sequential
  sublayers:
    - _target_: penzai.nn.Embedding
      vocab_size: ${training_data_generator.instance.vocab_size}
    - _target_: penzai.nn.TransformerBlock
      # ...
```

### Flax

Create a callable wrapper:
```python
def create_flax_model(vocab_size, **kwargs):
    import flax.linen as nn
    
    class FlaxModel(nn.Module):
        vocab_size: int
        
        @nn.compact
        def __call__(self, x):
            # Flax model definition
            return logits
    
    # Initialize and return callable
    model = FlaxModel(vocab_size)
    params = model.init(jax.random.PRNGKey(0), dummy_input)
    
    def apply_fn(x):
        return model.apply(params, x)
    
    return apply_fn
```

### Haiku

Transform to pure function:
```python
def create_haiku_model(vocab_size, **kwargs):
    import haiku as hk
    
    def model_fn(x):
        # Haiku model definition
        embed = hk.Embed(vocab_size, embed_size)
        # ...
        return logits
    
    # Transform to pure function
    model = hk.without_apply_rng(hk.transform(model_fn))
    params = model.init(jax.random.PRNGKey(0), dummy_input)
    
    def apply_fn(x):
        return model.apply(params, x)
    
    return apply_fn
```

## Best Practices

### 1. Input/Output Shapes

Always ensure:
- Input: `(batch_size, sequence_length)`
- Output: `(batch_size, sequence_length, vocab_size)`

```python
def validate_model(model, batch_size=2, seq_len=10, vocab_size=2):
    x = jnp.ones((batch_size, seq_len), dtype=jnp.int32)
    out = model(x)
    assert out.shape == (batch_size, seq_len, vocab_size)
```

### 2. Initialization

Use proper initialization:
```python
# For Equinox
key = jax.random.PRNGKey(seed)
model = eqx.filter_jit(model_fn)(key, vocab_size)

# For external libraries
model = library.create_model(vocab_size, key=key)
```

### 3. Type Checking

Verify protocol compliance:
```python
from simplexity.predictive_models import PredictiveModel

def create_model(...) -> PredictiveModel:
    model = build_model(...)
    assert isinstance(model, PredictiveModel)
    return model
```

### 4. Memory Efficiency

For large models:
```python
# Use gradient checkpointing
model = eqx.filter_checkpoint(model)

# Use mixed precision
model = amp.wrap_model(model)
```

## Common Patterns

### 1. Positional Encoding

```python
def add_positional_encoding(embed_fn, max_len=5000):
    def embed_with_pos(x):
        embedded = embed_fn(x)
        positions = jnp.arange(x.shape[1])
        pos_encoding = compute_sinusoidal_encoding(positions, embedded.shape[-1])
        return embedded + pos_encoding
    return embed_with_pos
```

### 2. Layer Normalization

```python
def create_normalized_model(vocab_size, hidden_size):
    return eqx.nn.Sequential([
        eqx.nn.Embedding(vocab_size, hidden_size),
        eqx.nn.LayerNorm(hidden_size),
        ProcessingLayers(...),
        eqx.nn.LayerNorm(hidden_size),
        eqx.nn.Linear(hidden_size, vocab_size),
    ])
```

### 3. Residual Connections

```python
class ResidualBlock(eqx.Module):
    layers: eqx.Module
    
    def __call__(self, x):
        return x + self.layers(x)
```

## Debugging Tips

### 1. Shape Debugging

```python
def debug_shapes(model):
    def wrapped_model(x):
        print(f"Input shape: {x.shape}")
        out = model(x)
        print(f"Output shape: {out.shape}")
        return out
    return wrapped_model
```

### 2. Gradient Checking

```python
def check_gradients(model, x):
    def loss_fn(model, x):
        logits = model(x)
        return jnp.mean(logits)
    
    grads = eqx.filter_grad(loss_fn)(model, x)
    # Check for NaN/Inf
    has_nan = any(jnp.any(jnp.isnan(g)) for g in jax.tree_leaves(grads))
    print(f"Has NaN gradients: {has_nan}")
```

### 3. Model Summary

```python
def print_model_summary(model):
    num_params = sum(x.size for x in jax.tree_leaves(eqx.filter(model, eqx.is_array)))
    print(f"Total parameters: {num_params:,}")
    
    # Print layer structure
    for name, layer in eqx.tree_at(lambda m: m, model).items():
        print(f"{name}: {type(layer).__name__}")
```
