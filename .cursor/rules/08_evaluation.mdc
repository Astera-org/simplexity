---
description: Model evaluation patterns, metrics computation, and computational mechanics evaluation
keywords: [evaluation, metrics, validation, computational mechanics, performance measurement]
use_when: Adding new metrics, understanding validation, implementing evaluation logic
load_priority: component
---
# Evaluation & Metrics

Simplexity provides comprehensive evaluation capabilities for assessing model performance on sequence prediction tasks.

## Evaluation Configuration

```yaml
# configs/evaluation/small.yaml
num_batches: 10
batch_size: 32
sequence_length: 128

# configs/evaluation/large.yaml  
num_batches: 100
batch_size: 64
sequence_length: 256
```

## Core Evaluation Functions

### Equinox Model Evaluation

```python
# simplexity/evaluation/evaluate_equinox_model.py
def evaluate(
    model: PredictiveModel,
    cfg: EvaluationConfig,
    data_generator: GenerativeProcess,
    metric_fns: Optional[Dict[str, Callable]] = None,
) -> Dict[str, float]:
    """Evaluate model and return metrics."""
```

### Penzai Model Evaluation

```python
# simplexity/evaluation/evaluate_penzai_model.py
def evaluate(
    model: pz.nn.Layer,
    cfg: EvaluationConfig,
    data_generator: GenerativeProcess,
    metric_fns: Optional[Dict[str, Callable]] = None,
) -> Dict[str, float]:
    """Evaluate Penzai model."""
```

## Built-in Metrics

### Cross-Entropy Loss

```python
def cross_entropy_loss(logits: jax.Array, targets: jax.Array) -> jax.Array:
    """Compute cross-entropy loss."""
    return optax.softmax_cross_entropy_with_integer_labels(
        logits, targets
    ).mean()
```

### Accuracy

```python
def accuracy(logits: jax.Array, targets: jax.Array) -> jax.Array:
    """Compute prediction accuracy."""
    predictions = jnp.argmax(logits, axis=-1)
    return (predictions == targets).mean()
```

### Perplexity

```python
def perplexity(logits: jax.Array, targets: jax.Array) -> jax.Array:
    """Compute perplexity (exp of cross-entropy)."""
    ce_loss = cross_entropy_loss(logits, targets)
    return jnp.exp(ce_loss)
```

## Custom Metrics

### Creating Custom Metrics

```python
# Define metric function
def top_k_accuracy(k: int = 5):
    def metric_fn(logits: jax.Array, targets: jax.Array) -> jax.Array:
        # Get top k predictions
        top_k_preds = jax.lax.top_k(logits, k)[1]
        
        # Check if target is in top k
        target_expanded = targets[..., None]
        in_top_k = (top_k_preds == target_expanded).any(axis=-1)
        
        return in_top_k.mean()
    
    return metric_fn

# Use in evaluation
metric_fns = {
    'loss': cross_entropy_loss,
    'accuracy': accuracy,
    'top_5_accuracy': top_k_accuracy(5),
}
```

### Sequence-Level Metrics

```python
def sequence_accuracy(logits: jax.Array, targets: jax.Array) -> jax.Array:
    """Compute full sequence accuracy."""
    predictions = jnp.argmax(logits, axis=-1)
    # All predictions in sequence must be correct
    return (predictions == targets).all(axis=-1).mean()

def edit_distance(logits: jax.Array, targets: jax.Array) -> jax.Array:
    """Compute average edit distance."""
    predictions = jnp.argmax(logits, axis=-1)
    
    @jax.vmap
    def single_edit_distance(pred, target):
        # Implement Levenshtein distance
        return compute_edit_distance(pred, target)
    
    distances = single_edit_distance(predictions, targets)
    return distances.mean()
```

## Evaluation Patterns

### Basic Evaluation Loop

```python
def evaluate_model(model, cfg, data_generator, key):
    """Complete evaluation function."""
    losses = []
    accuracies = []
    
    for batch_idx in range(cfg.num_batches):
        # Generate batch
        key, subkey = jax.random.split(key)
        states = jax.vmap(lambda _: data_generator.initial_state)(
            jnp.arange(cfg.batch_size)
        )
        _, observations = data_generator.generate(
            states, 
            jax.random.split(subkey, cfg.batch_size),
            cfg.sequence_length,
            return_all_states=False
        )
        
        # Model predictions
        inputs = observations[:, :-1]
        targets = observations[:, 1:]
        logits = model(inputs)
        
        # Compute metrics
        loss = cross_entropy_loss(logits, targets)
        acc = accuracy(logits, targets)
        
        losses.append(loss)
        accuracies.append(acc)
    
    return {
        'loss': jnp.mean(jnp.array(losses)),
        'accuracy': jnp.mean(jnp.array(accuracies)),
    }
```

### Advanced Evaluation

```python
@eqx.filter_jit
def evaluate_with_states(model, data_generator, observations, states):
    """Evaluate with access to ground truth states."""
    # Get model predictions
    inputs = observations[:, :-1]
    targets = observations[:, 1:]
    logits = model(inputs)
    
    # Standard metrics
    metrics = {
        'loss': cross_entropy_loss(logits, targets),
        'accuracy': accuracy(logits, targets),
    }
    
    # State-based metrics
    if hasattr(model, 'get_hidden_states'):
        hidden_states = model.get_hidden_states(inputs)
        
        # Compare with ground truth states
        state_similarity = compute_state_similarity(
            hidden_states, states[:, 1:]  # Align with targets
        )
        metrics['state_similarity'] = state_similarity
    
    return metrics
```

## Computational Mechanics Metrics

### Statistical Complexity

```python
def statistical_complexity(model, data_generator, sequence_length=1000):
    """Estimate statistical complexity of learned model."""
    # Generate long sequence
    key = jax.random.PRNGKey(0)
    state = data_generator.initial_state
    _, observations = data_generator.generate(
        state, key, sequence_length, False
    )
    
    # Get model's hidden states
    if hasattr(model, 'get_hidden_states'):
        hidden_states = model.get_hidden_states(observations[None, :])[0]
        
        # Compute entropy of hidden state distribution
        state_probs = estimate_state_distribution(hidden_states)
        complexity = -jnp.sum(state_probs * jnp.log2(state_probs + 1e-10))
        
        return complexity
    else:
        return None
```

### Predictive Information

```python
def predictive_information(logits, targets, future_steps=10):
    """Compute mutual information between predictions and future."""
    # Get prediction distributions
    pred_probs = jax.nn.softmax(logits)
    
    # Compute MI with future observations
    mi_values = []
    for t in range(future_steps):
        if t < targets.shape[1]:
            future_targets = targets[:, t:]
            current_preds = pred_probs[:, :-t if t > 0 else None]
            
            mi = mutual_information(current_preds, future_targets)
            mi_values.append(mi)
    
    return jnp.mean(jnp.array(mi_values))
```

## Evaluation During Training

### Validation Callback

```python
class ValidationCallback:
    def __init__(self, model, val_generator, val_cfg, logger):
        self.model = model
        self.val_generator = val_generator
        self.val_cfg = val_cfg
        self.logger = logger
        self.best_loss = float('inf')
    
    def __call__(self, epoch, current_model):
        # Run evaluation
        metrics = evaluate(
            current_model,
            self.val_cfg,
            self.val_generator
        )
        
        # Log metrics
        for name, value in metrics.items():
            self.logger.log_metrics({f'val/{name}': value}, step=epoch)
        
        # Track best model
        if metrics['loss'] < self.best_loss:
            self.best_loss = metrics['loss']
            self.logger.log_metrics({'val/best_loss': self.best_loss}, step=epoch)
            return True  # Signal to save checkpoint
        
        return False
```

### Early Stopping

```python
class EarlyStoppingEvaluator:
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_metric = None
        self.patience_counter = 0
    
    def should_stop(self, current_metric):
        if self.best_metric is None:
            self.best_metric = current_metric
            return False
        
        if current_metric < self.best_metric - self.min_delta:
            self.best_metric = current_metric
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            return self.patience_counter >= self.patience
```

## Efficient Evaluation

### Batched Evaluation

```python
@eqx.filter_jit
def evaluate_batched(model, all_observations):
    """Evaluate on pre-generated data efficiently."""
    # Process all batches at once
    inputs = all_observations[:, :, :-1]
    targets = all_observations[:, :, 1:]
    
    # Vectorize over batch dimension
    logits = jax.vmap(model)(inputs)
    
    # Compute metrics over all data
    losses = jax.vmap(cross_entropy_loss)(logits, targets)
    accs = jax.vmap(accuracy)(logits, targets)
    
    return {
        'loss': losses.mean(),
        'accuracy': accs.mean(),
        'loss_std': losses.std(),
        'accuracy_std': accs.std(),
    }
```

### Streaming Evaluation

```python
def evaluate_streaming(model, data_generator, total_sequences=1000):
    """Evaluate on streaming data without storing all in memory."""
    
    @jax.jit
    def eval_single_batch(observations):
        inputs = observations[:, :-1]
        targets = observations[:, 1:]
        logits = model(inputs)
        return {
            'loss': cross_entropy_loss(logits, targets),
            'accuracy': accuracy(logits, targets),
        }
    
    # Accumulate metrics
    metric_accumulator = defaultdict(list)
    
    key = jax.random.PRNGKey(0)
    for i in range(0, total_sequences, batch_size):
        # Generate batch
        key, subkey = jax.random.split(key)
        batch_size_current = min(batch_size, total_sequences - i)
        
        _, observations = generate_batch(
            data_generator, batch_size_current, seq_len, subkey
        )
        
        # Evaluate
        batch_metrics = eval_single_batch(observations)
        
        for name, value in batch_metrics.items():
            metric_accumulator[name].append(value)
    
    # Aggregate
    return {
        name: jnp.mean(jnp.array(values))
        for name, values in metric_accumulator.items()
    }
```

## Visualization and Analysis

### Metric Tracking

```python
class MetricTracker:
    def __init__(self):
        self.history = defaultdict(list)
    
    def update(self, metrics, step):
        for name, value in metrics.items():
            self.history[name].append((step, float(value)))
    
    def plot(self, metric_names=None):
        import matplotlib.pyplot as plt
        
        if metric_names is None:
            metric_names = list(self.history.keys())
        
        fig, axes = plt.subplots(len(metric_names), 1, figsize=(10, 4*len(metric_names)))
        if len(metric_names) == 1:
            axes = [axes]
        
        for ax, name in zip(axes, metric_names):
            steps, values = zip(*self.history[name])
            ax.plot(steps, values)
            ax.set_xlabel('Step')
            ax.set_ylabel(name)
            ax.set_title(f'{name} over time')
        
        plt.tight_layout()
        return fig
```

### Error Analysis

```python
def analyze_errors(model, data_generator, num_examples=100):
    """Analyze model errors in detail."""
    key = jax.random.PRNGKey(0)
    errors = []
    
    for i in range(num_examples):
        key, subkey = jax.random.split(key)
        
        # Generate single sequence
        state = data_generator.initial_state
        _, obs = data_generator.generate(state, subkey, 50, False)
        
        # Get predictions
        logits = model(obs[None, :-1])[0]
        preds = jnp.argmax(logits, axis=-1)
        targets = obs[1:]
        
        # Find errors
        error_positions = jnp.where(preds != targets)[0]
        
        if len(error_positions) > 0:
            errors.append({
                'sequence': obs,
                'predictions': preds,
                'error_positions': error_positions,
                'error_rate': len(error_positions) / len(targets),
            })
    
    # Analyze error patterns
    error_rates = [e['error_rate'] for e in errors]
    print(f"Average error rate: {np.mean(error_rates):.3f}")
    print(f"Sequences with errors: {len(errors)}/{num_examples}")
    
    return errors
```

## Best Practices

1. **Always use consistent evaluation data**: Set random seed for reproducibility
2. **Evaluate on multiple metrics**: Single metrics can be misleading
3. **Monitor variance**: Report standard deviations, not just means
4. **Use appropriate batch sizes**: Balance memory and statistical accuracy
5. **Save evaluation results**: Store metrics with checkpoints
6. **Implement domain-specific metrics**: Generic metrics may miss important aspects
