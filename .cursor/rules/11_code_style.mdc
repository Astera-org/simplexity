---
description: Project coding conventions, style guidelines, documentation standards, and git practices
globs: 
alwaysApply: false
---
# Code Style & Conventions

Simplexity follows specific conventions to maintain consistency and readability across the codebase.

## Python Style Guide

### General Guidelines

- Follow PEP 8 with modifications for JAX/scientific computing
- Use type hints for all public functions
- Maximum line length: 100 characters (relaxed from PEP 8's 79)
- Use descriptive variable names

### Imports

```python
# Standard library imports first
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Protocol, Tuple

# Third-party imports (grouped)
import jax
import jax.numpy as jnp
from jax import Array
import equinox as eqx
import penzai as pz
import optax
from hydra import compose, initialize
from omegaconf import DictConfig

# Local imports last
from simplexity.generative_processes import GenerativeProcess
from simplexity.predictive_models import PredictiveModel
from simplexity.utils.hydra import typed_instantiate
```

### Type Annotations

```python
# Use modern type hints
def train(
    model: PredictiveModel,
    cfg: TrainingConfig,
    data_generator: GenerativeProcess[State],
    logger: Logger,
) -> tuple[PredictiveModel, float]:
    """Train a model and return trained model with final loss."""
    pass

# Use Protocol for interfaces
class ModelPersister(Protocol):
    def save_checkpoint(self, model: Any, step: int) -> None: ...
    def load_checkpoint(self, step: int) -> Any: ...
```

## JAX-Specific Conventions

### Array Types

```python
# Use jax.Array instead of jnp.ndarray for type hints
def process_data(x: jax.Array) -> jax.Array:
    return jnp.sqrt(x)

# Use chex for array type checking in generative processes
import chex

def emit_observation(self, state: State, key: chex.PRNGKey) -> chex.Array:
    pass
```

### Pure Functions

```python
# Keep functions pure (no side effects)
def compute_loss(logits: jax.Array, targets: jax.Array) -> jax.Array:
    """Pure function for loss computation."""
    return optax.softmax_cross_entropy_with_integer_labels(
        logits, targets
    ).mean()

# Use filter_jit for Equinox modules
@eqx.filter_jit
def forward_pass(model: eqx.Module, x: jax.Array) -> jax.Array:
    return model(x)
```

### Random Keys

```python
# Always split keys explicitly
key, subkey = jax.random.split(key)

# Use descriptive names for split keys
key, data_key, model_key = jax.random.split(key, 3)

# Pass keys explicitly, never use global state
def generate_data(key: jax.random.PRNGKey, size: int) -> jax.Array:
    return jax.random.normal(key, (size,))
```

## Naming Conventions

### Variables and Functions

```python
# Use snake_case for variables and functions
learning_rate = 0.001
batch_size = 32

def calculate_metrics(predictions, targets):
    pass

# Use descriptive names
# Good
validation_loss = 0.5
num_hidden_states = 8

# Bad
val_l = 0.5
n = 8
```

### Classes

```python
# Use PascalCase for classes
class HiddenMarkovModel(GenerativeProcess):
    pass

class GRUPredictor(eqx.Module):
    pass

# Suffix with base class type when appropriate
class LocalPersister(ModelPersister):
    pass
```

### Constants

```python
# Use UPPER_SNAKE_CASE for constants
DEFAULT_BATCH_SIZE = 32
MAX_SEQUENCE_LENGTH = 1000
EPSILON = 1e-8
```

## Module Structure

### File Organization

```python
"""Module docstring explaining purpose.

This module implements X functionality for Y purpose.
"""

# Imports (grouped as shown above)
import ...

# Module-level constants
DEFAULT_VALUE = 42

# Type definitions
State = TypeVar("State")

# Classes/Protocols
class MyClass:
    """Class docstring."""
    pass

# Functions
def public_function():
    """Function docstring."""
    pass

def _private_helper():
    """Private functions start with underscore."""
    pass

# Module initialization (if needed)
__all__ = ["MyClass", "public_function"]
```

### Docstrings

```python
def train(
    model: PredictiveModel,
    cfg: TrainingConfig,
    data_generator: GenerativeProcess,
) -> tuple[PredictiveModel, float]:
    """Train a predictive model.
    
    Args:
        model: The model to train
        cfg: Training configuration
        data_generator: Source of training data
        
    Returns:
        Tuple of (trained_model, final_loss)
        
    Raises:
        ValueError: If configuration is invalid
        
    Example:
        >>> model = create_model()
        >>> trained_model, loss = train(model, cfg, generator)
    """
    pass

class GenerativeProcess(Protocol):
    """Abstract base class for generative processes.
    
    A generative process produces sequences of observations
    based on hidden states. Implementations should provide
    methods for generation, state transitions, and probability
    calculations.
    
    Attributes:
        vocab_size: Number of possible observations
        initial_state: Starting state distribution
    """
```

## Configuration Files

### YAML Style

```yaml
# Use descriptive keys
name: gru_rnn
instance:
  _target_: simplexity.predictive_models.gru_rnn.create_gru_rnn
  vocab_size: ${training_data_generator.instance.vocab_size}
  hidden_size: 128
  num_layers: 2
  dropout: 0.1  # Add comments for clarity

# Group related parameters
training:
  num_epochs: 50
  batch_size: 32
  
  optimizer:
    instance:
      _target_: optax.adam
      learning_rate: 0.001
```

### Structured Configs

```python
@dataclass
class TrainingConfig:
    """Training configuration.
    
    Attributes:
        num_epochs: Number of training epochs
        batch_size: Batch size for training
        sequence_length: Length of sequences
    """
    num_epochs: int = 10
    batch_size: int = 32
    sequence_length: int = 128
    gradient_clip_norm: Optional[float] = 1.0
```

## Error Handling

### Exceptions

```python
# Provide informative error messages
def load_checkpoint(self, step: int) -> Any:
    checkpoint_path = self.save_dir / f"checkpoint_{step}.eqx"
    
    if not checkpoint_path.exists():
        raise FileNotFoundError(
            f"Checkpoint not found at {checkpoint_path}. "
            f"Available checkpoints: {list(self.save_dir.glob('checkpoint_*.eqx'))}"
        )
    
    try:
        return eqx.tree_deserialise_leaves(checkpoint_path)
    except Exception as e:
        raise RuntimeError(
            f"Failed to load checkpoint from {checkpoint_path}: {e}"
        ) from e
```

### Validation

```python
# Validate inputs early
def create_model(vocab_size: int, hidden_size: int) -> PredictiveModel:
    if vocab_size <= 0:
        raise ValueError(f"vocab_size must be positive, got {vocab_size}")
    
    if hidden_size <= 0:
        raise ValueError(f"hidden_size must be positive, got {hidden_size}")
    
    # Proceed with creation
```

## Testing Conventions

### Test Naming

```python
# Test files mirror source files
# Source: simplexity/training/train_model.py
# Test: tests/training/test_train_model.py

# Test function names describe what is tested
def test_training_reduces_loss():
    """Test that training reduces loss over epochs."""
    pass

def test_invalid_config_raises_error():
    """Test that invalid configuration raises appropriate error."""
    pass
```

### Test Organization

```python
class TestHiddenMarkovModel:
    """Group related tests in classes."""
    
    @pytest.fixture
    def simple_hmm(self):
        """Fixtures provide test data."""
        return create_test_hmm()
    
    def test_generation(self, simple_hmm):
        """Each test focuses on one aspect."""
        pass
```

## Comments and Documentation

### Inline Comments

```python
# Use comments to explain why, not what
# Good: Normalize to prevent numerical instability
probabilities = probabilities / (jnp.sum(probabilities) + 1e-10)

# Bad: Divide by sum
probabilities = probabilities / jnp.sum(probabilities)

# Use comments for complex logic
# Apply Bayes' rule: P(state|obs) ∝ P(obs|state) * P(state)
posterior = likelihood * prior
posterior = posterior / jnp.sum(posterior)
```

### TODO Comments

```python
# Format: TODO(username): Description
# TODO(alice): Implement caching for performance
# TODO(bob): Add support for variable length sequences

# FIXME for bugs that need fixing
# FIXME: This fails when batch_size=1

# NOTE for important information
# NOTE: This assumes vocab_size is fixed during training
```

## Git Commit Messages

### Format

```
<type>: <subject>

<body>

<footer>
```

### Types

- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation changes
- `style`: Code style changes (formatting, missing semicolons, etc.)
- `refactor`: Code refactoring
- `test`: Adding or updating tests
- `chore`: Maintenance tasks

### Examples

```
feat: add transformer model support

- Implement transformer architecture in predictive_models
- Add configuration for transformer hyperparameters
- Update tests to include transformer model

Closes #123
```

```
fix: prevent NaN in probability normalization

Add small epsilon to denominator when normalizing
probability distributions to prevent division by zero.
```

## Pre-commit Hooks

### Setup

```bash
# Install pre-commit
pip install pre-commit

# Install hooks
pre-commit install
```

### Configuration (.pre-commit-config.yaml)

```yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      
  - repo: https://github.com/psf/black
    rev: 23.1.0
    hooks:
      - id: black
        language_version: python3.9
        
  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
        args: ["--profile", "black"]
```

## Code Review Checklist

Before submitting PR, ensure:

1. **Tests pass**: `uv run pytest`
2. **Type hints added**: For all public functions
3. **Docstrings updated**: For new/modified functions
4. **No commented code**: Remove or convert to TODOs
5. **Consistent naming**: Follow conventions above
6. **Error handling**: Appropriate exceptions with messages
7. **JAX best practices**: Pure functions, explicit keys
8. **Configuration**: Added to appropriate YAML files
