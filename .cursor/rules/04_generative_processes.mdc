---
description: Creating and using data generation components, GenerativeProcess protocol implementation
keywords: [generative processes, data generation, HMM, MSP, epsilon machines, protocols]
use_when: Adding new generative processes, understanding data flow, working with sequence generation
load_priority: component
---
# Generative Processes

Generative processes in Simplexity create sequences of observations based on underlying hidden states, providing both training data and ground truth for model evaluation.

## Core Concepts

### Abstract Base Class

All generative processes inherit from `GenerativeProcess[State]`:

```python
class GenerativeProcess(eqx.Module, Generic[State]):
    @property
    @abstractmethod
    def vocab_size(self) -> int: ...
    
    @property
    @abstractmethod
    def initial_state(self) -> State: ...
    
    @abstractmethod
    def emit_observation(self, state: State, key: chex.PRNGKey) -> chex.Array: ...
    
    @abstractmethod
    def transition_states(self, state: State, obs: chex.Array) -> State: ...
```

### State Management

States represent probability distributions over hidden states:
- **Prior**: State before observation
- **Posterior**: State after conditioning on observation
- **Belief State**: Current distribution over hidden states

## Built-in Processes

### 1. Hidden Markov Model (HMM)

Standard HMM with discrete states:

```yaml
# configs/generative_process/simple_hmm.yaml
name: simple_hmm
instance:
  _target_: simplexity.generative_processes.HiddenMarkovModel
  transition_matrix: [[0.9, 0.1], [0.1, 0.9]]
  emission_matrix: [[0.8, 0.2], [0.2, 0.8]]
  initial_distribution: [0.5, 0.5]
```

### 2. Mixed State Presentation (MSP)

Complex process with epsilon-machines:

```yaml
# configs/generative_process/mess3.yaml
name: mess3
instance:
  _target_: simplexity.generative_processes.builder.build_generative_process
  name: mess3
  builder_type: msp
  epsilon: 0.01
  edge_epsilon: 0.0
```

Available MSP types:
- `simple_msp`
- `noisy_copy`
- `hypercube`
- `mess3`
- `complex_msp`

### 3. Epsilon Machines

From computational mechanics literature:

```yaml
# configs/generative_process/even_ones.yaml
name: even_ones
instance:
  _target_: simplexity.generative_processes.builder.build_generative_process
  name: even_ones
  builder_type: epsilon_machine
```

Available epsilon machines:
- `even_ones`: Even number of consecutive 1s
- `rrxor`: Random-random XOR
- `tom_quantum`: Toms' quantum process
- And more...

## Creating New Processes

### Step 1: Define State Type

```python
from typing import NamedTuple
import jax.numpy as jnp

class MyState(NamedTuple):
    probabilities: jnp.ndarray  # Shape: (num_states,)
    memory: jnp.ndarray        # Additional state info
```

### Step 2: Implement Process

```python
from simplexity.generative_processes import GenerativeProcess
import equinox as eqx

class MyProcess(GenerativeProcess[MyState]):
    num_states: int
    transition_matrix: jnp.ndarray
    emission_matrix: jnp.ndarray
    
    def __init__(self, num_states: int, **kwargs):
        self.num_states = num_states
        self.transition_matrix = self._build_transitions()
        self.emission_matrix = self._build_emissions()
    
    @property
    def vocab_size(self) -> int:
        return self.emission_matrix.shape[1]
    
    @property
    def initial_state(self) -> MyState:
        return MyState(
            probabilities=jnp.ones(self.num_states) / self.num_states,
            memory=jnp.zeros(self.num_states)
        )
    
    def emit_observation(self, state: MyState, key: chex.PRNGKey) -> chex.Array:
        # Sample from emission distribution
        emission_probs = state.probabilities @ self.emission_matrix
        return jax.random.choice(key, self.vocab_size, p=emission_probs)
    
    def transition_states(self, state: MyState, obs: chex.Array) -> MyState:
        # Bayesian update based on observation
        likelihood = self.emission_matrix[:, obs]
        posterior = state.probabilities * likelihood
        posterior = posterior / jnp.sum(posterior)
        
        # Transition to next state
        next_probs = posterior @ self.transition_matrix
        
        return MyState(
            probabilities=next_probs,
            memory=update_memory(state.memory, obs)
        )
```

### Step 3: Add Configuration

Create `configs/generative_process/my_process.yaml`:

```yaml
name: my_process
instance:
  _target_: simplexity.generative_processes.MyProcess
  num_states: 4
  # Additional parameters...
```

Update `configs/generative_process/config.py`:

```python
@dataclass
class MyProcessConfig:
    _target_: str = "simplexity.generative_processes.MyProcess"
    num_states: int = 4

# Register with ConfigStore
cs.store(
    group="generative_process",
    name="my_process",
    node=GenerativeProcessConfig(
        name="my_process",
        instance=MyProcessConfig()
    ),
)
```

## Using the Builder Pattern

For complex processes, use the builder:

```python
# In simplexity/generative_processes/builder.py
def build_my_process(config: Dict[str, Any]) -> GenerativeProcess:
    # Complex construction logic
    if config.get("variant") == "simple":
        return MySimpleProcess(...)
    else:
        return MyComplexProcess(...)

# Register builder
BUILDERS["my_process"] = build_my_process
```

Configuration:

```yaml
instance:
  _target_: simplexity.generative_processes.builder.build_generative_process
  name: my_process
  builder_type: my_process
  variant: complex
  # Additional parameters...
```

## Data Generation Patterns

### Batch Generation

```python
# Generate batch of sequences
def generate_batch(process, batch_size, seq_len, key):
    keys = jax.random.split(key, batch_size)
    initial_states = jax.vmap(lambda _: process.initial_state)(keys)
    
    states, observations = process.generate(
        initial_states, 
        keys, 
        seq_len,
        return_all_states=True
    )
    return states, observations
```

### Streaming Generation

```python
# Generate sequences on-the-fly
def data_generator(process, seq_len, key):
    while True:
        key, subkey = jax.random.split(key)
        state = process.initial_state
        _, obs = process.generate(state, subkey, seq_len, False)
        yield obs
```

## Testing Processes

### Verify Stochastic Properties

```python
def test_stationary_distribution(process):
    # Generate long sequence
    key = jax.random.PRNGKey(0)
    state = process.initial_state
    _, obs = process.generate(state, key, 10000, False)
    
    # Check empirical distribution
    counts = jnp.bincount(obs, length=process.vocab_size)
    empirical_dist = counts / len(obs)
    
    # Compare with theoretical
    assert jnp.allclose(empirical_dist, expected_dist, atol=0.01)
```

### Test Markov Property

```python
def test_markov_property(process):
    # Verify P(X_t | X_{t-1}, ..., X_0) = P(X_t | X_{t-1})
    # Implementation depends on process type
    pass
```

## Advanced Features

### 1. Noisy Processes

Add observation noise:

```python
def add_noise(process, epsilon):
    class NoisyProcess(GenerativeProcess):
        def emit_observation(self, state, key):
            key1, key2 = jax.random.split(key)
            true_obs = process.emit_observation(state, key1)
            
            # With probability epsilon, emit random symbol
            is_noise = jax.random.uniform(key2) < epsilon
            noise_obs = jax.random.choice(key2, self.vocab_size)
            
            return jnp.where(is_noise, noise_obs, true_obs)
    
    return NoisyProcess()
```

### 2. Composed Processes

Combine multiple processes:

```python
class ComposedProcess(GenerativeProcess):
    processes: List[GenerativeProcess]
    
    def emit_observation(self, state, key):
        # Emit from multiple processes
        # Combine observations somehow
        pass
```

### 3. Non-Stationary Processes

Time-varying dynamics:

```python
class NonStationaryProcess(GenerativeProcess):
    def transition_states(self, state, obs, time_step):
        # Transition matrix depends on time
        T = self.get_transition_matrix(time_step)
        # Apply transition...
```

## Best Practices

### 1. Numerical Stability

```python
# Avoid numerical issues in probability updates
posterior = state.probabilities * likelihood
posterior = posterior / (jnp.sum(posterior) + 1e-10)  # Add small epsilon

# Use log-space for very small probabilities
log_posterior = jnp.log(state.probabilities + 1e-10) + jnp.log(likelihood + 1e-10)
log_posterior = log_posterior - jax.nn.logsumexp(log_posterior)
```

### 2. Efficient Implementation

```python
# Precompute matrices when possible
@eqx.filter_jit
def precompute_emissions(self):
    # Cache computations
    return computed_values

# Use JAX transformations
transition_fn = jax.vmap(self.transition_states, in_axes=(0, 0))
```

### 3. Validation

Always validate:
- Probability distributions sum to 1
- Transition matrices are stochastic
- Observations are in valid range
- States are properly normalized

## Common Issues

### 1. Degeneracy

When all probability mass concentrates on one state:

```python
# Add small uniform noise to prevent degeneracy
probabilities = 0.99 * probabilities + 0.01 / self.num_states
```

### 2. Initialization

Poor initialization can affect convergence:

```python
# Use informed initialization when possible
def initial_state(self):
    if self.has_stationary_dist:
        return self.stationary_distribution
    else:
        return uniform_distribution(self.num_states)
```
