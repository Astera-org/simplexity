---
description: Running experiments, hyperparameter optimization with Optuna, and MLflow tracking
keywords: [experiments, optuna, hyperparameters, sweeps, mlflow, workflow]
use_when: Setting up experiments, running hyperparameter sweeps, debugging experiment issues
load_priority: medium
---
# Experiment Workflow

This guide covers running experiments, from single training runs to large-scale hyperparameter optimization.

## Entry Points

### 1. Single Training Run

Use `train_model.py` for individual experiments:

```bash
# Default configuration
uv run python simplexity/train_model.py

# With overrides
uv run python simplexity/train_model.py \
    predictive_model=transformer \
    training.num_epochs=50 \
    seed=42
```

**Key features:**
- Trains one model with specified config
- Logs metrics to MLflow
- Saves checkpoints to disk/S3
- Evaluates on validation set

### 2. Hyperparameter Sweeps

Use `run_experiment.py` for optimization:

```bash
# Run Optuna sweep
uv run python simplexity/run_experiment.py --multirun

# Parallel execution
uv run python simplexity/run_experiment.py --multirun hydra.sweeper.n_jobs=4
```

**Key features:**
- Optuna-based hyperparameter search
- Parallel trial execution
- Automatic metric tracking
- Returns best configuration

## Experiment Configuration

### Basic Experiment Setup

In `experiment.yaml`:

```yaml
defaults:
  - _self_
  - generative_process@training_data_generator: mess3
  - generative_process@validation_data_generator: mess3
  - predictive_model: gru_rnn
  - persistence: local_persister
  - logging: mlflow_logger
  - train: medium
  - override hydra/sweeper: optuna

seed: 0
experiment_name: ${predictive_model.name}_${training_data_generator.name}
run_name: ${now:%Y-%m-%d_%H-%M-%S}_${experiment_name}_${seed}
```

### Sweep Configuration

Define search spaces:

```yaml
hydra:
  sweeper:
    direction: minimize  # or maximize
    n_trials: 20
    n_jobs: 1
    storage: null  # or SQLite URL for persistence
    study_name: my_study
    
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: ${seed}
      n_startup_trials: 10
    
    params:
      # Continuous parameters
      train.optimizer.instance.learning_rate: tag(log, interval(1e-4, 1e-1))
      
      # Discrete choices
      train.batch_size: choice(16, 32, 64, 128)
      
      # Integer ranges
      predictive_model.instance.hidden_size: int(interval(32, 256))
      predictive_model.instance.num_layers: int(interval(1, 4))
```

## Workflow Patterns

### 1. Development Workflow

Start small, iterate quickly:

```bash
# Quick test with minimal data
uv run python simplexity/train_model.py \
    training=small \
    training.num_epochs=5

# If working, scale up
uv run python simplexity/train_model.py \
    training=medium \
    training.num_epochs=50
```

### 2. Systematic Experimentation

```bash
# Step 1: Baseline
uv run python simplexity/train_model.py \
    experiment_name=baseline

# Step 2: Architecture search
uv run python simplexity/run_experiment.py --multirun \
    experiment_name=architecture_search \
    'hydra.sweeper.params.predictive_model.instance.hidden_size=int(interval(64,512))' \
    'hydra.sweeper.params.predictive_model.instance.num_layers=int(interval(1,6))'

# Step 3: Learning rate tuning with best architecture
uv run python simplexity/run_experiment.py --multirun \
    experiment_name=lr_tuning \
    predictive_model.instance.hidden_size=256 \
    predictive_model.instance.num_layers=3 \
    'hydra.sweeper.params.train.optimizer.instance.learning_rate=tag(log,interval(1e-5,1e-2))'
```

### 3. Reproducibility Workflow

```bash
# Save configuration
uv run python simplexity/train_model.py --cfg job > configs/best_config.yaml

# Reproduce exact run
uv run python simplexity/train_model.py \
    --config-path=. \
    --config-name=best_config \
    seed=42
```

## Output Organization

Hydra creates structured outputs:

```
outputs/
└── 2024-03-15/
    ├── 10-30-00/  # Single run
    │   ├── .hydra/
    │   │   ├── config.yaml
    │   │   ├── hydra.yaml
    │   │   └── overrides.yaml
    │   └── train_model.log
    └── 11-00-00/  # Multirun
        ├── 0/
        ├── 1/
        └── optimization_results.yaml
```

## MLflow Integration

### Viewing Results

```bash
# Start MLflow UI
mlflow ui

# View at http://localhost:5000
```

### Tracking Metrics

The framework automatically logs:
- Training/validation loss per epoch
- Learning rate schedules
- Model architecture details
- Hyperparameters
- Final metrics

### Comparing Runs

Use MLflow UI to:
1. Sort runs by metrics
2. Compare parameters
3. Visualize training curves
4. Download artifacts

## Advanced Patterns

### 1. Conditional Sweeps

```yaml
# Only search dropout if model supports it
hydra:
  sweeper:
    params:
      +predictive_model.instance.dropout: 
        if: ${predictive_model.name} == "transformer"
        then: interval(0.0, 0.5)
```

### 2. Multi-Objective Optimization

```python
@hydra.main(config_path="configs", config_name="experiment.yaml")
def run_experiment(cfg: Config) -> tuple[float, float]:
    # ... training code ...
    return val_loss, -val_accuracy  # Minimize loss, maximize accuracy
```

### 3. Checkpoint Resume

```yaml
# Resume from checkpoint
predictive_model:
  load_checkpoint_step: 1000
  
# Continue training
training:
  num_epochs: 100  # Total epochs
  resume_from_checkpoint: true
```

### 4. Custom Metrics

```python
# In training loop
logger.log_metrics({
    "custom/metric": computed_value,
    "custom/other": other_value
}, step=epoch)
```

## Best Practices

### 1. Experiment Naming

Use descriptive names:
```bash
experiment_name="${date}_${model}_${dataset}_${objective}"
```

### 2. Parameter Ranges

- Start with wide ranges
- Narrow based on results
- Use log scale for learning rates
- Consider parameter interactions

### 3. Computational Efficiency

```yaml
# Efficient sweep setup
hydra:
  sweeper:
    n_jobs: 4  # Parallel trials
    
# Early stopping in training config
training:
  early_stopping_patience: 10
  early_stopping_min_delta: 0.001
```

### 4. Result Analysis

```python
# Load best hyperparameters
from hydra import compose, initialize
from omegaconf import OmegaConf

with initialize(config_path="outputs/2024-03-15/11-00-00/.hydra"):
    cfg = compose(config_name="config")
    print(OmegaConf.to_yaml(cfg))
```

## Troubleshooting

### Common Issues

1. **OOM Errors**: Reduce batch size or model size
2. **Slow Training**: Check data loading, enable JIT
3. **No Improvement**: Verify learning rate range
4. **Divergence**: Check gradient clipping settings

### Debug Commands

```bash
# Dry run to check config
uv run python simplexity/train_model.py --cfg job

# Single epoch test
uv run python simplexity/train_model.py training.num_epochs=1

# Verbose logging
uv run python simplexity/train_model.py hydra.verbose=true
```
